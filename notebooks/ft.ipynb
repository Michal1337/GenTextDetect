{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "#model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.lm_head = nn.Identity() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# special_tokens = {\n",
    "#     \"additional_special_tokens\": [\"<|S1|>\", \"<|S2|>\", \"<|S3|>\", \"<|S4|>\", \"<|S5|>\"]\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.add_special_tokens(special_tokens)\n",
    "# model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze only the embeddings\n",
    "# embedding_layer = model.get_input_embeddings()\n",
    "# for param in embedding_layer.parameters():\n",
    "#     param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceClassifier(nn.Module):\n",
    "    def __init__(self, base_model, num_labels):\n",
    "        super(SentenceClassifier, self).__init__()\n",
    "        self.base_model = base_model\n",
    "        self.classifier = nn.Linear(self.base_model.config.hidden_size * 2, num_labels)\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        B, T, C = outputs.logits.shape\n",
    "        \n",
    "        all_tokens_hidden = outputs.logits # (B, T, C)\n",
    "        last_token_hidden = outputs.logits[:, -1, :] # (B, C)\n",
    "        last_token_hidden = last_token_hidden.unsqueeze(1).expand(B, T, C)\n",
    "\n",
    "        combined_representation = torch.cat((all_tokens_hidden, last_token_hidden), dim=-1)\n",
    "        logits = self.classifier(combined_representation)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SentenceClassifier(model, num_labels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4097"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get number of parameters\n",
    "sum(p.numel() for p in classifier.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = \"<|finetune_right_pad_id|>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128004"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get id of pad token\n",
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer):\n",
    "        \"\"\"\n",
    "        texts: list of multi-sentence strings.\n",
    "        labels: list of lists containing one label per sentence.\n",
    "        \"\"\"\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        return {\"text\": text, \"label\": label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"../data/datasets/test2.csv\")\n",
    "\n",
    "dataset = SentenceDataset(df[\"text\"].tolist()[:1000], df[\"label\"].tolist()[:1000], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, tokenizer):\n",
    "    texts = [item[\"text\"] for item in batch]\n",
    "    labels = [item[\"label\"] for item in batch]\n",
    "    encodings = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    labels_padded = [torch.where(t == 0, torch.tensor(-100), torch.tensor(label)) for t, label in zip(encodings[\"attention_mask\"], labels)]\n",
    "    labels_padded = torch.cat(labels_padded)\n",
    "    encodings[\"labels\"] = labels_padded\n",
    "\n",
    "    return encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn= lambda batch: collate_fn(batch, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        num_layers: int,\n",
    "        nhead: int,\n",
    "        max_seq_length: int,\n",
    "        vocab_size: int,\n",
    "        pad_token_id: int,\n",
    "        num_labels: int,\n",
    "    ) -> None:\n",
    "        super(BaselineClassifier, self).__init__()\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.token_embedding = nn.Embedding(\n",
    "            vocab_size, d_model, padding_idx=pad_token_id\n",
    "        )\n",
    "        self.pos_embedding = nn.Embedding(max_seq_length, d_model)\n",
    "        decoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(decoder_layer, num_layers=num_layers)\n",
    "        self.classifier = nn.Linear(d_model * 2, num_labels)\n",
    "\n",
    "    def forward(self, token_ids: torch.tensor) -> torch.tensor:\n",
    "        batch_size, seq_len = token_ids.shape\n",
    "\n",
    "        token_emb = self.token_embedding(token_ids)\n",
    "        pos_ids = torch.arange(seq_len, device=token_ids.device).unsqueeze(0)\n",
    "        pos_emb = self.pos_embedding(pos_ids)\n",
    "        embeddings = token_emb + pos_emb\n",
    "\n",
    "        causal_mask = torch.triu(\n",
    "            torch.ones(seq_len, seq_len, device=token_ids.device, dtype=torch.bool),\n",
    "            diagonal=1,\n",
    "        )\n",
    "\n",
    "        pad_mask = token_ids.eq(self.pad_token_id)  # shape: (batch_size, seq_len)\n",
    "\n",
    "        output = self.transformer(\n",
    "            embeddings, mask=causal_mask, src_key_padding_mask=pad_mask\n",
    "        )\n",
    "\n",
    "        B, T, C = output.shape\n",
    "        all_tokens_hidden = output  # (B, T, C)\n",
    "        last_token_hidden = output[:, -1, :]  # (B, C)\n",
    "        last_token_hidden = last_token_hidden.unsqueeze(1).expand(B, T, C)\n",
    "\n",
    "        combined_representation = torch.cat(\n",
    "            (all_tokens_hidden, last_token_hidden), dim=-1\n",
    "        )\n",
    "        logits = self.classifier(combined_representation)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = BaselineClassifier(512, 6, 8, 512, len(tokenizer), tokenizer.pad_token_id, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.distributed as dist\n",
    "\n",
    "# set up DDP (distributed data parallel).\n",
    "# torchrun command sets the env variables RANK, LOCAL_RANK, and WORLD_SIZE\n",
    "ddp = int(os.environ.get('RANK', -1)) != -1 # is this a ddp run?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import pandas as pd\n",
    "\n",
    "def setup_process(rank, world_size, backend='nccl'):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "    dist.init_process_group(backend, rank=rank, world_size=world_size)\n",
    "\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels_batch = batch['labels'].to(device)\n",
    "            mask = labels_batch != -100\n",
    "\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            logits = logits.view(-1)[mask]\n",
    "            labels_batch = labels_batch.view(-1)[mask].float()\n",
    "\n",
    "            preds = torch.sigmoid(logits) > 0.5\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels_batch.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary')\n",
    "    return acc, precision, recall, f1\n",
    "\n",
    "def train(rank, world_size, dataloader, val_dataset, model, num_epochs=3):\n",
    "    setup_process(rank, world_size)\n",
    "    device = torch.device(f'cuda:{rank}' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    # Wrap model with DDP\n",
    "    model = DDP(model, device_ids=[rank])\n",
    "\n",
    "    # Create a DistributedSampler for the training dataset\n",
    "    train_sampler = DistributedSampler(dataloader.dataset, num_replicas=world_size, rank=rank)\n",
    "    train_loader = DataLoader(dataloader.dataset, batch_size=32, sampler=train_sampler, shuffle=False)\n",
    "    \n",
    "    # Similarly, setup validation dataloader (no need for shuffling)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    best_f1 = 0.0\n",
    "    history_rows = []\n",
    "    model_save_path = \"best_model.pt\"\n",
    "    os.makedirs(\"training_logs\", exist_ok=True)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        # Set the sampler epoch for shuffling\n",
    "        train_sampler.set_epoch(epoch)\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=f\"Rank {rank} Epoch {epoch+1}/{num_epochs}\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels_batch = batch['labels'].to(device)\n",
    "            mask = labels_batch != -100\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            logits = logits.view(-1)[mask]\n",
    "            labels_batch = labels_batch.view(-1)[mask].float()\n",
    "\n",
    "            loss = criterion(logits, labels_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.sigmoid(logits).detach().cpu() > 0.5\n",
    "            all_preds.extend(preds.numpy())\n",
    "            all_labels.extend(labels_batch.cpu().numpy())\n",
    "\n",
    "        train_acc = accuracy_score(all_labels, all_preds)\n",
    "        _, _, train_f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary')\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "\n",
    "        # Evaluate only on rank 0 to avoid redundant validation\n",
    "        if rank == 0:\n",
    "            val_acc, _, _, val_f1 = evaluate(model, val_loader, device)\n",
    "            if val_f1 > best_f1:\n",
    "                best_f1 = val_f1\n",
    "                # Save the model (unwrap DDP)\n",
    "                torch.save(model.module.state_dict(), model_save_path)\n",
    "                print(f\"Saved new best model (F1: {best_f1:.4f})\")\n",
    "            print(\n",
    "                f\"Epoch {epoch+1}/{num_epochs} \"\n",
    "                f\"| Train Loss: {avg_loss:.4f} \"\n",
    "                f\"| Train F1: {train_f1:.4f} \"\n",
    "                f\"| Val F1: {val_f1:.4f}\"\n",
    "            )\n",
    "            history_rows.append({\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"train_loss\": avg_loss,\n",
    "                \"train_accuracy\": train_acc,\n",
    "                \"train_f1\": train_f1,\n",
    "                \"val_f1\": val_f1\n",
    "            })\n",
    "\n",
    "    if rank == 0:\n",
    "        df_history = pd.DataFrame(history_rows)\n",
    "        df_history.to_csv(\"training_logs/training_history.csv\", index=False)\n",
    "        print(\"Training history saved to training_logs/training_history.csv\")\n",
    "\n",
    "    cleanup()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    world_size = torch.cuda.device_count()\n",
    "    # Assume `dataloader` is defined and contains your training dataset.\n",
    "    # Also assume `val_dataset` is your validation dataset.\n",
    "    # And assume `classifier` is your model.\n",
    "    mp.spawn(train, args=(world_size, dataloader, val_dataset, classifier), nprocs=world_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
