{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "#model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.lm_head = nn.Identity() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# special_tokens = {\n",
    "#     \"additional_special_tokens\": [\"<|S1|>\", \"<|S2|>\", \"<|S3|>\", \"<|S4|>\", \"<|S5|>\"]\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.add_special_tokens(special_tokens)\n",
    "# model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze only the embeddings\n",
    "# embedding_layer = model.get_input_embeddings()\n",
    "# for param in embedding_layer.parameters():\n",
    "#     param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceClassifier(nn.Module):\n",
    "    def __init__(self, base_model, num_labels):\n",
    "        super(SentenceClassifier, self).__init__()\n",
    "        self.base_model = base_model\n",
    "        self.classifier = nn.Linear(self.base_model.config.hidden_size * 2, num_labels)\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        B, T, C = outputs.logits.shape\n",
    "        \n",
    "        all_tokens_hidden = outputs.logits # (B, T, C)\n",
    "        last_token_hidden = outputs.logits[:, -1, :] # (B, C)\n",
    "        last_token_hidden = last_token_hidden.unsqueeze(1).expand(B, T, C)\n",
    "\n",
    "        combined_representation = torch.cat((all_tokens_hidden, last_token_hidden), dim=-1)\n",
    "        logits = self.classifier(combined_representation)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SentenceClassifier(model, num_labels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4097"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get number of parameters\n",
    "sum(p.numel() for p in classifier.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = \"<|finetune_right_pad_id|>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer):\n",
    "        \"\"\"\n",
    "        texts: list of multi-sentence strings.\n",
    "        labels: list of lists containing one label per sentence.\n",
    "        \"\"\"\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        return {\"text\": text, \"label\": label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"../data/datasets/test2.csv\")\n",
    "\n",
    "dataset = SentenceDataset(df[\"text\"].tolist()[:1000], df[\"label\"].tolist()[:1000], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    texts = [item[\"text\"] for item in batch]\n",
    "    labels = [item[\"label\"] for item in batch]\n",
    "    encodings = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    labels_padded = [torch.where(t == 0, torch.tensor(-100), torch.tensor(label)) for t, label in zip(encodings[\"attention_mask\"], labels)]\n",
    "    labels_padded = torch.cat(labels_padded)\n",
    "    encodings[\"labels\"] = labels_padded\n",
    "\n",
    "    return encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Model hyperparameters\n",
    "d_model = 512\n",
    "vocab_size = 10000\n",
    "seq_length = 10\n",
    "batch_size = 32\n",
    "nhead = 8\n",
    "num_layers = 6\n",
    "pad_token_id = 0\n",
    "\n",
    "class TransformerDecoderWithEmbeddings(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, seq_length, num_layers, nhead, pad_token_id):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model, padding_idx=pad_token_id)\n",
    "        self.pos_embedding = nn.Embedding(seq_length, d_model)\n",
    "        decoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(decoder_layer, num_layers=num_layers)\n",
    "        self.pad_token_id = pad_token_id\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        batch_size, seq_len = token_ids.shape\n",
    "        \n",
    "        # Compute token and positional embeddings\n",
    "        token_emb = self.token_embedding(token_ids)\n",
    "        pos_ids = torch.arange(seq_len, device=token_ids.device).unsqueeze(0)\n",
    "        pos_emb = self.pos_embedding(pos_ids)\n",
    "        embeddings = token_emb + pos_emb\n",
    "\n",
    "        # Create a boolean causal mask: True for positions that should be masked.\n",
    "        causal_mask = torch.triu(torch.ones(seq_len, seq_len, device=token_ids.device, dtype=torch.bool), diagonal=1)\n",
    "\n",
    "        # Create a padding mask: True at positions where the token is a pad token.\n",
    "        pad_mask = token_ids.eq(self.pad_token_id)  # shape: (batch_size, seq_len)\n",
    "\n",
    "        # Pass the embeddings along with both masks.\n",
    "        output = self.transformer(embeddings, mask=causal_mask, src_key_padding_mask=pad_mask)\n",
    "        return output\n",
    "\n",
    "# Instantiate the model.\n",
    "model = TransformerDecoderWithEmbeddings(vocab_size, d_model, seq_length, num_layers, nhead, pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randint(1, vocab_size, (batch_size, seq_length))\n",
    "x[:, -2:] = pad_token_id  # Adding padding at the last two positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.6574e+00,  5.1426e-01,  2.1099e+00,  ..., -7.0132e-01,\n",
       "          -7.6854e-01, -1.8805e+00],\n",
       "         [ 8.1144e-01, -1.1377e-01,  1.7264e+00,  ..., -8.3019e-01,\n",
       "           4.0725e-01, -1.3923e+00],\n",
       "         [ 3.6627e-01,  2.2241e-01,  2.0570e+00,  ...,  4.6055e-01,\n",
       "          -6.0812e-01, -8.8407e-01],\n",
       "         ...,\n",
       "         [-6.5247e-02,  9.3704e-01,  9.5043e-01,  ...,  7.0408e-01,\n",
       "          -4.8024e-02, -1.0185e+00],\n",
       "         [ 3.8166e-01,  9.0517e-01,  3.9536e-01,  ...,  3.0520e-01,\n",
       "          -9.4121e-01, -8.3781e-01],\n",
       "         [ 1.1688e+00,  5.3544e-01,  8.1944e-01,  ...,  1.3912e+00,\n",
       "           1.7287e-01, -8.7220e-01]],\n",
       "\n",
       "        [[ 1.0444e+00, -2.7527e-01,  4.2235e-01,  ...,  1.3844e-01,\n",
       "          -6.0575e-01, -2.9296e-01],\n",
       "         [-2.0117e-01, -3.4369e-01,  6.3022e-01,  ...,  6.4732e-01,\n",
       "          -5.9118e-02, -2.7027e-01],\n",
       "         [ 1.9697e-01, -1.0001e-01,  5.3323e-01,  ...,  6.9925e-01,\n",
       "          -5.7076e-01, -7.7109e-01],\n",
       "         ...,\n",
       "         [ 7.7518e-01,  3.7093e-01, -2.9092e-01,  ...,  1.3132e+00,\n",
       "           9.9931e-02, -5.9508e-01],\n",
       "         [ 7.2648e-01,  2.2477e-01, -3.4517e-01,  ...,  7.0203e-01,\n",
       "          -1.7339e-01, -7.9752e-01],\n",
       "         [ 1.1224e+00,  2.9647e-01,  1.2488e-01,  ...,  2.2021e+00,\n",
       "           7.3474e-01, -3.2669e-01]],\n",
       "\n",
       "        [[ 1.5313e+00, -6.0831e-01,  3.6359e-01,  ...,  5.8344e-01,\n",
       "          -8.7538e-01,  6.2664e-01],\n",
       "         [ 8.9070e-01, -9.5634e-01, -3.0252e-02,  ...,  4.1526e-01,\n",
       "          -6.5963e-01,  1.1789e-02],\n",
       "         [ 5.4237e-01,  1.3198e-01,  4.0347e-01,  ...,  1.1373e+00,\n",
       "          -1.1642e+00, -7.7529e-02],\n",
       "         ...,\n",
       "         [ 1.4277e+00,  4.3244e-02, -4.5712e-01,  ...,  1.2529e+00,\n",
       "          -7.1822e-01, -2.2104e-01],\n",
       "         [ 4.5716e-01,  5.0784e-01, -8.6580e-01,  ...,  8.6183e-01,\n",
       "          -1.1150e+00, -6.3731e-01],\n",
       "         [ 1.7910e+00,  5.6607e-02, -1.0486e-01,  ...,  2.1592e+00,\n",
       "          -3.0417e-02, -3.6178e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-1.0508e-01, -6.3601e-01,  1.7504e+00,  ...,  3.3561e-01,\n",
       "           5.3638e-01, -2.4284e+00],\n",
       "         [ 3.4959e-01, -8.0265e-01,  8.8516e-01,  ...,  7.3532e-01,\n",
       "           2.0689e+00, -1.5564e+00],\n",
       "         [ 2.9403e-01,  4.2109e-02,  4.2017e-01,  ...,  7.4776e-01,\n",
       "          -4.5013e-01, -9.0132e-01],\n",
       "         ...,\n",
       "         [ 2.0052e-01,  1.2268e-01,  5.2286e-02,  ...,  1.7156e+00,\n",
       "          -5.5971e-02, -3.8002e-01],\n",
       "         [ 1.1140e-03,  1.2309e-01,  2.1501e-01,  ...,  4.7459e-01,\n",
       "          -5.7343e-01, -5.9905e-01],\n",
       "         [ 1.0763e+00,  4.1320e-02,  1.1268e+00,  ...,  2.4273e+00,\n",
       "           8.6112e-01, -3.3274e-01]],\n",
       "\n",
       "        [[ 8.6381e-02,  4.9788e-01,  9.1582e-01,  ...,  4.7889e-01,\n",
       "          -2.4636e-02, -1.5483e+00],\n",
       "         [ 7.2033e-02, -7.2834e-01,  1.0134e+00,  ...,  1.0027e+00,\n",
       "           3.5280e-01, -2.7010e-01],\n",
       "         [-7.1386e-01, -4.6997e-01,  1.0464e+00,  ...,  1.2674e+00,\n",
       "          -7.7776e-01,  1.1300e+00],\n",
       "         ...,\n",
       "         [-1.7018e-01, -1.2456e-01, -3.5549e-01,  ...,  1.1124e+00,\n",
       "          -2.1601e-01,  4.7581e-01],\n",
       "         [-1.2801e+00,  2.7559e-01, -4.1125e-01,  ...,  6.9216e-01,\n",
       "          -4.9804e-01,  4.0919e-01],\n",
       "         [ 2.9155e-01,  3.4940e-01,  3.2503e-01,  ...,  2.2955e+00,\n",
       "           4.9415e-01,  2.7370e-01]],\n",
       "\n",
       "        [[ 2.8332e-01,  5.8969e-01,  7.2253e-02,  ...,  4.8691e-01,\n",
       "          -4.7578e-01, -1.4067e+00],\n",
       "         [ 9.1334e-01,  9.1029e-01,  7.4888e-01,  ...,  4.0573e-01,\n",
       "          -1.1654e-01, -1.0687e+00],\n",
       "         [ 3.3665e-01, -6.6223e-04,  3.4921e-01,  ...,  1.2685e+00,\n",
       "          -3.6994e-01, -8.4177e-01],\n",
       "         ...,\n",
       "         [ 3.3859e-01,  1.1654e-01,  1.5889e-01,  ...,  7.5820e-01,\n",
       "          -1.6042e+00, -1.2920e+00],\n",
       "         [ 3.9945e-01,  3.2202e-01, -1.9182e-01,  ...,  2.5728e-01,\n",
       "          -1.3363e+00, -1.4831e+00],\n",
       "         [ 1.1106e+00,  2.2153e-01, -1.6855e-01,  ...,  1.9664e+00,\n",
       "           1.5419e-01, -1.1324e+00]]], grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(x)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [04:46<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 0.0434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [05:22<00:00,  2.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3, Loss: 0.0036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/125 [00:05<05:20,  2.61s/it]"
     ]
    }
   ],
   "source": [
    "optimizer = optim.AdamW(classifier.parameters(), lr=5e-5)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "device = \"cuda\"\n",
    "classifier.to(device)\n",
    "classifier.train()\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(dataloader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels_batch = batch['labels'].to(device)\n",
    "        mask = labels_batch != -100\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = classifier(input_ids, attention_mask)\n",
    "        logits = logits.view(-1)[mask]\n",
    "        labels_batch = labels_batch.view(-1)[mask].float()\n",
    "\n",
    "        loss = criterion(logits, labels_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
