{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-13 02:49:10 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from itertools import islice\n",
    "from transformers import AutoTokenizer\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "SEED = 1337\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target         ids                          date      flag  \\\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../../data/data_raw/tweets.csv\", encoding='latin-1', names=[\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([\"target\", \"ids\", \"date\", \"flag\", \"user\"], axis=1, inplace=True)\n",
    "df.to_csv(\"../../data/data_human/tweets.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(iterable, batch_size):\n",
    "    \"\"\"Splits an iterable into smaller batches.\"\"\"\n",
    "    iterable = iter(iterable)\n",
    "    while batch := list(islice(iterable, batch_size)):\n",
    "        yield batch\n",
    "\n",
    "def save_to_csv(path, prompts, responses, temperature, top_p, top_k):\n",
    "    \"\"\"Saves prompts, responses and sampling parameters to a CSV file.\"\"\"\n",
    "    with open(path, mode='a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        for prompt, response in zip(prompts, responses):\n",
    "            writer.writerow([prompt, response, temperature, top_p, top_k])\n",
    "\n",
    "def generate_responses(model, prompts_tokens, sampling_params):\n",
    "    \"\"\"Generate a batch of outputs using vLLM with customizable sampling parameters.\"\"\"\n",
    "    outputs = model.generate(sampling_params=sampling_params, prompt_token_ids=prompts_tokens, use_tqdm=False)\n",
    "    \n",
    "    return [sample.outputs[0].text for sample in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PROMPT = [{\"role\": \"system\", \"content\": \"You are a helpful asistant for rewritting tweets. Based on provided tweet generate a similar one. MAKE SURE TO REPLAY ONLY WITH THE SIMILAR TWEET.\"},\n",
    "                {\"role\": \"user\", \"content\": \"Tweet: \\n {tweet} \\n\"},\n",
    "                {\"role\": \"assistant\", \"content\": \"Similar tweet: \\n\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    [\n",
    "        BASE_PROMPT[0],  # The system message\n",
    "        {\"role\": \"user\", \"content\": BASE_PROMPT[1][\"content\"].format(tweet=tweet)},  # Formatted user message\n",
    "        BASE_PROMPT[2]  # The assistant message\n",
    "    ]\n",
    "    for tweet in df[\"text\"].values\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = [\n",
    "    SamplingParams(temperature=0.0, top_p=1.0, top_k=-1, max_tokens=10_000, seed=SEED),  # Pure Greedy (fully deterministic)\n",
    "    SamplingParams(temperature=0.2, top_p=1.0, top_k=-1, max_tokens=10_000, seed=SEED),  # Highly Deterministic\n",
    "    SamplingParams(temperature=0.5, top_p=0.95, top_k=100, max_tokens=10_000, seed=SEED), # Mildly Deterministic but Flexible\n",
    "    SamplingParams(temperature=0.7, top_p=0.9, top_k=50, max_tokens=10_000, seed=SEED),  # Balanced and Natural\n",
    "    SamplingParams(temperature=0.9, top_p=0.8, top_k=40, max_tokens=10_000, seed=SEED),  # Slightly More Diverse but Coherent\n",
    "    SamplingParams(temperature=1.0, top_p=0.95, top_k=30, max_tokens=10_000, seed=SEED), # Default Creative Mode\n",
    "    SamplingParams(temperature=1.2, top_p=0.7, top_k=20, max_tokens=10_000, seed=SEED),  # Highly Creative\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "llms = [\"meta-llama/Llama-3.2-1B-Instruct\"]\n",
    "batch_size = 8\n",
    "base_path = \"../../data/data_ai/tweets/tweets_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200000 [00:00<?, ?it/s]/tmp/ipykernel_658/3024968881.py:17: DeprecationWarning: The keyword arguments {'prompt_token_ids'} are deprecated and will be removed in a future update. Please use the 'prompts' parameter instead.\n",
      "  responses = generate_responses(model, prompts_tokens, params)\n",
      "  0%|          | 2/200000 [00:02<81:14:05,  1.46s/it]\n"
     ]
    }
   ],
   "source": [
    "for llm in llms:\n",
    "    #model = LLM(model=llm, dtype=\"half\", max_model_len = 10_000)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(llm)\n",
    "    csv_path = f\"{base_path}{llm.split('/')[-1]}.csv\"\n",
    "\n",
    "\n",
    "    # init csv file\n",
    "    with open(csv_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"prompt\", \"response\", \"temperature\", \"top_p\", \"top_k\"])\n",
    "\n",
    "    cnt = 0\n",
    "    for batch in tqdm(batchify(prompts, batch_size), total=len(prompts) // batch_size):\n",
    "        params = random.choice(sampling_params)\n",
    "        prompts_tokens = tokenizer.apply_chat_template(batch)\n",
    "        prompts_text = tokenizer.batch_decode(prompts_tokens)\n",
    "        responses = generate_responses(model, prompts_tokens, params)\n",
    "        save_to_csv(csv_path, prompts_text, responses, params.temperature, params.top_p, params.top_k)\n",
    "        cnt += 1\n",
    "        if cnt > 2:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 13 Feb 2025\\n\\nYou are a helpful asistant for rewritting tweets. Based on provided tweet generate a similar one. MAKE SURE TO REPLAY ONLY WITH THE SIMILAR TWEET.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nTweet: \\n Hollis' death scene will hurt me severely to watch on film  wry is directors cut not out now? \\n Similar tweet:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nSimilar tweet:<|eot_id|>\",\n",
       " '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 13 Feb 2025\\n\\nYou are a helpful asistant for rewritting tweets. Based on provided tweet generate a similar one. MAKE SURE TO REPLAY ONLY WITH THE SIMILAR TWEET.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nTweet: \\n about to file taxes  \\n Similar tweet:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nSimilar tweet:<|eot_id|>',\n",
       " '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 13 Feb 2025\\n\\nYou are a helpful asistant for rewritting tweets. Based on provided tweet generate a similar one. MAKE SURE TO REPLAY ONLY WITH THE SIMILAR TWEET.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nTweet: \\n @LettyA ahh ive always wanted to see rent  love the soundtrack!! \\n Similar tweet:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nSimilar tweet:<|eot_id|>',\n",
       " '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 13 Feb 2025\\n\\nYou are a helpful asistant for rewritting tweets. Based on provided tweet generate a similar one. MAKE SURE TO REPLAY ONLY WITH THE SIMILAR TWEET.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nTweet: \\n @FakerPattyPattz Oh dear. Were you drinking out of the forgotten table drinks?  \\n Similar tweet:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nSimilar tweet:<|eot_id|>',\n",
       " \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 13 Feb 2025\\n\\nYou are a helpful asistant for rewritting tweets. Based on provided tweet generate a similar one. MAKE SURE TO REPLAY ONLY WITH THE SIMILAR TWEET.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nTweet: \\n @alydesigns i was out most of the day so didn't get much done  \\n Similar tweet:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nSimilar tweet:<|eot_id|>\",\n",
       " \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 13 Feb 2025\\n\\nYou are a helpful asistant for rewritting tweets. Based on provided tweet generate a similar one. MAKE SURE TO REPLAY ONLY WITH THE SIMILAR TWEET.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nTweet: \\n one of my friend called me, and asked to meet with her at Mid Valley today...but i've no time *sigh*  \\n Similar tweet:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nSimilar tweet:<|eot_id|>\",\n",
       " '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 13 Feb 2025\\n\\nYou are a helpful asistant for rewritting tweets. Based on provided tweet generate a similar one. MAKE SURE TO REPLAY ONLY WITH THE SIMILAR TWEET.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nTweet: \\n @angry_barista I baked you a cake but I ated it  \\n Similar tweet:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nSimilar tweet:<|eot_id|>',\n",
       " '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 13 Feb 2025\\n\\nYou are a helpful asistant for rewritting tweets. Based on provided tweet generate a similar one. MAKE SURE TO REPLAY ONLY WITH THE SIMILAR TWEET.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nTweet: \\n this week is not going as i had hoped  \\n Similar tweet:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nSimilar tweet:<|eot_id|>']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|start_header_id|>assistant<|end_header_id|>\\n\\n\"RIP Hollis. The thought of his death scene being released on film still sends chills down my spine. Was it really necessary for a director\\'s cut, considering it was never intended for release?\"',\n",
       " '<|start_header_id|>assistant<|end_header_id|>\\n\\n\"The wait is over! Soon I\\'ll be filing my taxes, hoping I\\'ve done my part for the IRS and made the most of this tax season. #taxseason #filingtaxes\"',\n",
       " '<|start_header_id|>assistant<|end_header_id|>\\n\\n@NetflixCanHelp \"Ahah cant wait to see Rent again! The soundtrack is EVERYTHING\"',\n",
       " '<|start_header_id|>assistant<|end_header_id|>\\n\\n@KennySaoSweeeeet I think I saw you sipping from the forgotten table drinks again...',\n",
       " '<|start_header_id|>assistant<|end_header_id|>\\n\\n@designlover I was too busy most of the day to focus on design projects',\n",
       " '<|start_header_id|>assistant<|end_header_id|>\\n\\n\"One of my friends asked me to meet up at Mid Valley today, but I\\'ve got a ton of work to do *sigh*...\"',\n",
       " '<|start_header_id|>assistant<|end_header_id|>\\n\\n@disappointed_diner I baked you a cake but added too much icing',\n",
       " '<|start_header_id|>assistant<|end_header_id|>\\n\\n\"This week hasn\\'t lived up to my expectations. Still hoping for a better outcome tomorrow.\"']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = responses[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'assistant\\n\\n\"Sleep tight tonight... just can\\'t wait to see you again tomorrow morning\"'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove special tokens and return only the tweet using tokenizer\n",
    "tokenizer.decode(tokenizer.encode(tmp), skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Sleep tight tonight... just can't wait to see you again tomorrow morning\"\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\\\"Sleep tight tonight... just can't wait to see you again tomorrow morning\\\"\"\n",
    "\n",
    "# Define a pattern to match from <|start_header_id|> to <|end_header_id|> (inclusive)\n",
    "pattern = r\"<\\|start_header_id\\|>.*?<\\|end_header_id\\|>\"\n",
    "\n",
    "# Use re.sub to remove the entire matched block; flags=re.DOTALL makes '.' match newline characters\n",
    "cleaned_text = re.sub(pattern, \"\", text, flags=re.DOTALL).strip()\n",
    "\n",
    "print(cleaned_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
