{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-18 16:49:19 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from itertools import islice\n",
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "SEED = 1337\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>body</th>\n",
       "      <th>controversiality</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gameofthrones</td>\n",
       "      <td>Your submission has been automatically removed...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aww</td>\n",
       "      <td>Dont squeeze her with you massive hand, you me...</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gaming</td>\n",
       "      <td>It's pretty well known and it was a paid produ...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>news</td>\n",
       "      <td>You know we have laws against that currently c...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>politics</td>\n",
       "      <td>Yes, there is a difference between gentle supp...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       subreddit                                               body  \\\n",
       "0  gameofthrones  Your submission has been automatically removed...   \n",
       "1            aww  Dont squeeze her with you massive hand, you me...   \n",
       "2         gaming  It's pretty well known and it was a paid produ...   \n",
       "3           news  You know we have laws against that currently c...   \n",
       "4       politics  Yes, there is a difference between gentle supp...   \n",
       "\n",
       "   controversiality  score  \n",
       "0                 0      1  \n",
       "1                 0     19  \n",
       "2                 0      3  \n",
       "3                 0     10  \n",
       "4                 0      1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../../data/data_raw/reddit.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subreddit           0\n",
       "body                0\n",
       "controversiality    0\n",
       "score               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"body_length\"] = df[\"body\"].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>body</th>\n",
       "      <th>controversiality</th>\n",
       "      <th>score</th>\n",
       "      <th>body_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>499710</th>\n",
       "      <td>worldnews</td>\n",
       "      <td>LMAO, are you fucking high or just a bad liar?...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336158</th>\n",
       "      <td>worldnews</td>\n",
       "      <td>The CIA-MI6 plot did not succeed. It was not t...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>9958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582922</th>\n",
       "      <td>teenagers</td>\n",
       "      <td>🍞🍞🍞🍞🍞🍞🍞🍞🍞🍞🍞🍞🍞 🍞🍞🍞🍞🍞🍞🍞🍞🍞🍞🍞🍞🍞 🍞🍞🍞🍞🍞🍞🍞🍞🍞🍞🍞🍞🍞 🍞🍞🍞🍞...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>9897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546077</th>\n",
       "      <td>marvelstudios</td>\n",
       "      <td>Here's my take on it:\\n\\n**Time Travel.** Here...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>9846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205919</th>\n",
       "      <td>leagueoflegends</td>\n",
       "      <td>Part 2:\\n\\n### [](#c-tahmkench) Tahm Kench\\n*P...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222970</th>\n",
       "      <td>gaming</td>\n",
       "      <td>W H A T</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187057</th>\n",
       "      <td>nfl</td>\n",
       "      <td>W O A H</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114404</th>\n",
       "      <td>teenagers</td>\n",
       "      <td>S I P P</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326698</th>\n",
       "      <td>dankmemes</td>\n",
       "      <td>O H N O</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670182</th>\n",
       "      <td>teenagers</td>\n",
       "      <td>S a m e</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              subreddit                                               body  \\\n",
       "499710        worldnews  LMAO, are you fucking high or just a bad liar?...   \n",
       "336158        worldnews  The CIA-MI6 plot did not succeed. It was not t...   \n",
       "582922        teenagers  🍞🍞🍞🍞🍞🍞🍞🍞🍞🍞🍞🍞🍞 🍞🍞🍞🍞🍞🍞🍞🍞🍞🍞🍞🍞🍞 🍞🍞🍞🍞🍞🍞🍞🍞🍞🍞🍞🍞🍞 🍞🍞🍞🍞...   \n",
       "546077    marvelstudios  Here's my take on it:\\n\\n**Time Travel.** Here...   \n",
       "205919  leagueoflegends  Part 2:\\n\\n### [](#c-tahmkench) Tahm Kench\\n*P...   \n",
       "...                 ...                                                ...   \n",
       "222970           gaming                                            W H A T   \n",
       "187057              nfl                                            W O A H   \n",
       "114404        teenagers                                            S I P P   \n",
       "326698        dankmemes                                            O H N O   \n",
       "670182        teenagers                                            S a m e   \n",
       "\n",
       "        controversiality  score  body_length  \n",
       "499710                 0      1        10106  \n",
       "336158                 0      2         9958  \n",
       "582922                 0      3         9897  \n",
       "546077                 0      3         9846  \n",
       "205919                 0      1         9525  \n",
       "...                  ...    ...          ...  \n",
       "222970                 0      1            7  \n",
       "187057                 0      1            7  \n",
       "114404                 0      1            7  \n",
       "326698                 0      1            7  \n",
       "670182                 0      2            7  \n",
       "\n",
       "[1000000 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(\"body_length\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>body</th>\n",
       "      <th>controversiality</th>\n",
       "      <th>score</th>\n",
       "      <th>body_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gameofthrones</td>\n",
       "      <td>Your submission has been automatically removed...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aww</td>\n",
       "      <td>Dont squeeze her with you massive hand, you me...</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gaming</td>\n",
       "      <td>It's pretty well known and it was a paid produ...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>news</td>\n",
       "      <td>You know we have laws against that currently c...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>politics</td>\n",
       "      <td>Yes, there is a difference between gentle supp...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999867</th>\n",
       "      <td>trashy</td>\n",
       "      <td>I'm a mostly heartless asshole &amp;amp; that stil...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999870</th>\n",
       "      <td>trashy</td>\n",
       "      <td>LOL. He also said something about turning them...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999871</th>\n",
       "      <td>trashy</td>\n",
       "      <td>Yeah I love helping and being close to my fami...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999876</th>\n",
       "      <td>trashy</td>\n",
       "      <td>As a Canadian lefty I have to say I find Ameri...</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999877</th>\n",
       "      <td>trashy</td>\n",
       "      <td>My friend got herself a sugar daddy who asks f...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>670000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            subreddit                                               body  \\\n",
       "0       gameofthrones  Your submission has been automatically removed...   \n",
       "1                 aww  Dont squeeze her with you massive hand, you me...   \n",
       "2              gaming  It's pretty well known and it was a paid produ...   \n",
       "3                news  You know we have laws against that currently c...   \n",
       "4            politics  Yes, there is a difference between gentle supp...   \n",
       "...               ...                                                ...   \n",
       "999867         trashy  I'm a mostly heartless asshole &amp; that stil...   \n",
       "999870         trashy  LOL. He also said something about turning them...   \n",
       "999871         trashy  Yeah I love helping and being close to my fami...   \n",
       "999876         trashy  As a Canadian lefty I have to say I find Ameri...   \n",
       "999877         trashy  My friend got herself a sugar daddy who asks f...   \n",
       "\n",
       "        controversiality  score  body_length  \n",
       "0                      0      1          545  \n",
       "1                      0     19           55  \n",
       "2                      0      3          390  \n",
       "3                      0     10          111  \n",
       "4                      0      1          101  \n",
       "...                  ...    ...          ...  \n",
       "999867                 0      2           68  \n",
       "999870                 0      1          180  \n",
       "999871                 0      1          255  \n",
       "999876                 0     11          277  \n",
       "999877                 0      3           96  \n",
       "\n",
       "[670000 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df[\"body_length\"] > 50]\n",
    "df = df[df[\"subreddit\"] != \"Pikabu\"]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(22041)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated(subset=\"body\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset=\"body\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(iterable, batch_size):\n",
    "    \"\"\"Splits an iterable into smaller batches.\"\"\"\n",
    "    iterable = iter(iterable)\n",
    "    while batch := list(islice(iterable, batch_size)):\n",
    "        yield batch\n",
    "\n",
    "def save_to_csv(path, prompts, responses, temperature, top_p, top_k):\n",
    "    \"\"\"Saves prompts, responses and sampling parameters to a CSV file.\"\"\"\n",
    "    with open(path, mode='a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        for prompt, response in zip(prompts, responses):\n",
    "            writer.writerow([prompt, response, temperature, top_p, top_k])\n",
    "\n",
    "def generate_responses(model, prompts, sampling_params):\n",
    "    \"\"\"Generate a batch of outputs using vLLM with customizable sampling parameters.\"\"\"\n",
    "    outputs = model.chat(prompts, sampling_params=sampling_params, use_tqdm=False)\n",
    "    \n",
    "    return [sample.outputs[0].text.replace('\"', '') for sample in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PROMPT = [{\"role\": \"system\", \"content\": \"You are a helpful asistant for rewritting reddit comments. Based on provided comment and subreddit name, on which the comment was posted, generate a similar one. MAKE SURE TO REPLAY ONLY WITH THE SIMILAR COMMENT.\"},\n",
    "                {\"role\": \"user\", \"content\": \"Comment: \\n {comment} \\n Subreddit: {subreddit}\"},\n",
    "                {\"role\": \"assistant\", \"content\": \"Similar comment: \\n\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    [\n",
    "        BASE_PROMPT[0],  # The system message\n",
    "        {\"role\": \"user\", \"content\": BASE_PROMPT[1][\"content\"].format(comment=comment, subreddit=subreddit)},  # Formatted user message\n",
    "        BASE_PROMPT[2]  # The assistant message\n",
    "    ]\n",
    "    for comment, subreddit in df[[\"body\", \"subreddit\"]].values\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7813it [04:14, 30.73it/s]                          \n"
     ]
    }
   ],
   "source": [
    "lens = []\n",
    "batch_size = 128\n",
    "for prompts_batch in tqdm(batchify(prompts, batch_size), total=len(prompts) // batch_size):\n",
    "    tokens = tokenizer.apply_chat_template(prompts_batch)\n",
    "    lens.extend([len(token) for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "too_large = [i for i, l in enumerate(lens) if l > 32_768]\n",
    "too_large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([\"subreddit\", \"controversiality\", \"score\"], axis=1, inplace=True)\n",
    "df.to_csv(\"../../data/data_human/reddit.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = [\n",
    "    SamplingParams(temperature=0.0, top_p=1.0, top_k=-1, max_tokens=30_000, seed=SEED),  # Pure Greedy (fully deterministic)\n",
    "    SamplingParams(temperature=0.2, top_p=1.0, top_k=-1, max_tokens=30_000, seed=SEED),  # Highly Deterministic\n",
    "    SamplingParams(temperature=0.5, top_p=0.95, top_k=100, max_tokens=30_000, seed=SEED), # Mildly Deterministic but Flexible\n",
    "    SamplingParams(temperature=0.7, top_p=0.9, top_k=50, max_tokens=30_000, seed=SEED),  # Balanced and Natural\n",
    "    SamplingParams(temperature=0.9, top_p=0.8, top_k=40, max_tokens=30_000, seed=SEED),  # Slightly More Diverse but Coherent\n",
    "    SamplingParams(temperature=1.0, top_p=0.95, top_k=30, max_tokens=30_000, seed=SEED), # Default Creative Mode\n",
    "    SamplingParams(temperature=1.2, top_p=0.7, top_k=20, max_tokens=30_000, seed=SEED),  # Highly Creative\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "llms = [\"meta-llama/Llama-3.2-1B-Instruct\"]\n",
    "batch_size = 8\n",
    "base_path = \"../../data/data_ai/reddit/reddit_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 02-15 07:11:42 config.py:2386] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 02-15 07:11:51 config.py:542] This model supports multiple tasks: {'embed', 'generate', 'score', 'reward', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 02-15 07:11:51 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='meta-llama/Llama-3.2-1B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-1B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=10000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-3.2-1B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "WARNING 02-15 07:11:53 interface.py:284] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "INFO 02-15 07:11:53 cuda.py:179] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 02-15 07:11:53 cuda.py:227] Using XFormers backend.\n",
      "INFO 02-15 07:11:55 model_runner.py:1110] Starting to load model meta-llama/Llama-3.2-1B-Instruct...\n",
      "INFO 02-15 07:11:55 weight_utils.py:252] Using model weights format ['*.safetensors']\n",
      "INFO 02-15 07:11:56 weight_utils.py:297] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad0fe8d7580f4782bbc47a5b9fb83155",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-15 07:12:32 model_runner.py:1115] Loading model weights took 2.3185 GB\n",
      "INFO 02-15 07:12:34 worker.py:267] Memory profiling takes 2.19 seconds\n",
      "INFO 02-15 07:12:34 worker.py:267] the current vLLM instance can use total_gpu_memory (6.00GiB) x gpu_memory_utilization (0.90) = 5.40GiB\n",
      "INFO 02-15 07:12:34 worker.py:267] model weights take 2.32GiB; non_torch_memory takes 0.04GiB; PyTorch activation peak memory takes 1.21GiB; the rest of the memory reserved for KV Cache is 1.84GiB.\n",
      "INFO 02-15 07:12:35 executor_base.py:110] # CUDA blocks: 3761, # CPU blocks: 8192\n",
      "INFO 02-15 07:12:35 executor_base.py:115] Maximum concurrency for 10000 tokens per request: 6.02x\n",
      "INFO 02-15 07:12:39 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:30<00:00,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-15 07:13:10 model_runner.py:1562] Graph capturing finished in 30 secs, took 0.12 GiB\n",
      "INFO 02-15 07:13:10 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 37.99 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/125000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-15 07:13:10 chat_utils.py:332] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/125000 [00:05<91:42:54,  2.64s/it]\n"
     ]
    }
   ],
   "source": [
    "for llm, quant in llms:\n",
    "    model = LLM(model=llm, dtype=\"half\", max_model_len = 10_000, quantization=quant)\n",
    "    csv_path = f\"{base_path}{llm.split('/')[-1]}.csv\"\n",
    "\n",
    "\n",
    "    # init csv file\n",
    "    with open(csv_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"prompt\", \"response\", \"temperature\", \"top_p\", \"top_k\"])\n",
    "\n",
    "    cnt = 0\n",
    "    for prompts_batch in tqdm(batchify(prompts, batch_size), total=len(prompts) // batch_size):\n",
    "        params = random.choice(sampling_params)\n",
    "        responses = generate_responses(model, prompts_batch, params)\n",
    "        save_to_csv(csv_path, prompts_batch, responses, params.temperature, params.top_p, params.top_k)\n",
    "        cnt += 1\n",
    "        if cnt > 2:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>response</th>\n",
       "      <th>temperature</th>\n",
       "      <th>top_p</th>\n",
       "      <th>top_k</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'role': 'system', 'content': 'You are a help...</td>\n",
       "      <td>*This is a major plot twist that completely ch...</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.7</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{'role': 'system', 'content': 'You are a help...</td>\n",
       "      <td>Don't squeeze her with your giant hand, you me...</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.7</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[{'role': 'system', 'content': 'You are a help...</td>\n",
       "      <td>It's no surprise that gaming influencers and b...</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.7</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[{'role': 'system', 'content': 'You are a help...</td>\n",
       "      <td>You're referring to the 1994 Assault Weapons B...</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.7</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[{'role': 'system', 'content': 'You are a help...</td>\n",
       "      <td>Yes, there's a difference between a nuanced ap...</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.7</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0  [{'role': 'system', 'content': 'You are a help...   \n",
       "1  [{'role': 'system', 'content': 'You are a help...   \n",
       "2  [{'role': 'system', 'content': 'You are a help...   \n",
       "3  [{'role': 'system', 'content': 'You are a help...   \n",
       "4  [{'role': 'system', 'content': 'You are a help...   \n",
       "\n",
       "                                            response  temperature  top_p  \\\n",
       "0  *This is a major plot twist that completely ch...          1.2    0.7   \n",
       "1  Don't squeeze her with your giant hand, you me...          1.2    0.7   \n",
       "2  It's no surprise that gaming influencers and b...          1.2    0.7   \n",
       "3  You're referring to the 1994 Assault Weapons B...          1.2    0.7   \n",
       "4  Yes, there's a difference between a nuanced ap...          1.2    0.7   \n",
       "\n",
       "   top_k  \n",
       "0     20  \n",
       "1     20  \n",
       "2     20  \n",
       "3     20  \n",
       "4     20  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../../data/data_ai/reddit/reddit_Llama-3.2-1B-Instruct.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
