{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-15 04:37:42 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from itertools import islice\n",
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "SEED = 1337\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>commentBody</th>\n",
       "      <th>articleID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Here is something I think is fraudulent that v...</td>\n",
       "      <td>nyt://article/69a7090b-9f36-569e-b5ab-b0ba5bb3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@magicisnotreal  I have used my VA loan option...</td>\n",
       "      <td>nyt://article/69a7090b-9f36-569e-b5ab-b0ba5bb3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@magi\\n\\nWhy would someone take out a VA loan ...</td>\n",
       "      <td>nyt://article/69a7090b-9f36-569e-b5ab-b0ba5bb3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@JD\\nOut here in the Alabama of the PNW they w...</td>\n",
       "      <td>nyt://article/69a7090b-9f36-569e-b5ab-b0ba5bb3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@magicisnotreal  just a guess but I doubt that...</td>\n",
       "      <td>nyt://article/69a7090b-9f36-569e-b5ab-b0ba5bb3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         commentBody  \\\n",
       "0  Here is something I think is fraudulent that v...   \n",
       "1  @magicisnotreal  I have used my VA loan option...   \n",
       "2  @magi\\n\\nWhy would someone take out a VA loan ...   \n",
       "3  @JD\\nOut here in the Alabama of the PNW they w...   \n",
       "4  @magicisnotreal  just a guess but I doubt that...   \n",
       "\n",
       "                                           articleID  \n",
       "0  nyt://article/69a7090b-9f36-569e-b5ab-b0ba5bb3...  \n",
       "1  nyt://article/69a7090b-9f36-569e-b5ab-b0ba5bb3...  \n",
       "2  nyt://article/69a7090b-9f36-569e-b5ab-b0ba5bb3...  \n",
       "3  nyt://article/69a7090b-9f36-569e-b5ab-b0ba5bb3...  \n",
       "4  nyt://article/69a7090b-9f36-569e-b5ab-b0ba5bb3...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../../data/data_raw/nyt-comments-2020.csv\", usecols=[\"commentBody\", \"articleID\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>uniqueID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Congress could do much more to protect America...</td>\n",
       "      <td>nyt://article/69a7090b-9f36-569e-b5ab-b0ba5bb3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Christina Iverson and Jeff Chen ring in the Ne...</td>\n",
       "      <td>nyt://article/9edddb54-0aa3-5835-a833-d311a76f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All year long, Earth passes through streams of...</td>\n",
       "      <td>nyt://article/04bc90f0-b20b-511c-b5bb-3ce13194...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Never miss an eclipse, a meteor shower, a rock...</td>\n",
       "      <td>nyt://interactive/5b58d876-9351-50af-9b41-a312...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A year full of highs and lows in space just en...</td>\n",
       "      <td>nyt://article/bd8647b3-8ec6-50aa-95cf-2b81ed12...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            abstract  \\\n",
       "0  Congress could do much more to protect America...   \n",
       "1  Christina Iverson and Jeff Chen ring in the Ne...   \n",
       "2  All year long, Earth passes through streams of...   \n",
       "3  Never miss an eclipse, a meteor shower, a rock...   \n",
       "4  A year full of highs and lows in space just en...   \n",
       "\n",
       "                                            uniqueID  \n",
       "0  nyt://article/69a7090b-9f36-569e-b5ab-b0ba5bb3...  \n",
       "1  nyt://article/9edddb54-0aa3-5835-a833-d311a76f...  \n",
       "2  nyt://article/04bc90f0-b20b-511c-b5bb-3ce13194...  \n",
       "3  nyt://interactive/5b58d876-9351-50af-9b41-a312...  \n",
       "4  nyt://article/bd8647b3-8ec6-50aa-95cf-2b81ed12...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_articles = pd.read_csv(\"../../data/data_raw/nyt-articles-2020.csv\", usecols=[\"abstract\", \"uniqueID\"])\n",
    "df_articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>commentBody</th>\n",
       "      <th>articleID</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Here is something I think is fraudulent that v...</td>\n",
       "      <td>nyt://article/69a7090b-9f36-569e-b5ab-b0ba5bb3...</td>\n",
       "      <td>Congress could do much more to protect America...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@magicisnotreal  I have used my VA loan option...</td>\n",
       "      <td>nyt://article/69a7090b-9f36-569e-b5ab-b0ba5bb3...</td>\n",
       "      <td>Congress could do much more to protect America...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@magi\\n\\nWhy would someone take out a VA loan ...</td>\n",
       "      <td>nyt://article/69a7090b-9f36-569e-b5ab-b0ba5bb3...</td>\n",
       "      <td>Congress could do much more to protect America...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@JD\\nOut here in the Alabama of the PNW they w...</td>\n",
       "      <td>nyt://article/69a7090b-9f36-569e-b5ab-b0ba5bb3...</td>\n",
       "      <td>Congress could do much more to protect America...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@magicisnotreal  just a guess but I doubt that...</td>\n",
       "      <td>nyt://article/69a7090b-9f36-569e-b5ab-b0ba5bb3...</td>\n",
       "      <td>Congress could do much more to protect America...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         commentBody  \\\n",
       "0  Here is something I think is fraudulent that v...   \n",
       "1  @magicisnotreal  I have used my VA loan option...   \n",
       "2  @magi\\n\\nWhy would someone take out a VA loan ...   \n",
       "3  @JD\\nOut here in the Alabama of the PNW they w...   \n",
       "4  @magicisnotreal  just a guess but I doubt that...   \n",
       "\n",
       "                                           articleID  \\\n",
       "0  nyt://article/69a7090b-9f36-569e-b5ab-b0ba5bb3...   \n",
       "1  nyt://article/69a7090b-9f36-569e-b5ab-b0ba5bb3...   \n",
       "2  nyt://article/69a7090b-9f36-569e-b5ab-b0ba5bb3...   \n",
       "3  nyt://article/69a7090b-9f36-569e-b5ab-b0ba5bb3...   \n",
       "4  nyt://article/69a7090b-9f36-569e-b5ab-b0ba5bb3...   \n",
       "\n",
       "                                            abstract  \n",
       "0  Congress could do much more to protect America...  \n",
       "1  Congress could do much more to protect America...  \n",
       "2  Congress could do much more to protect America...  \n",
       "3  Congress could do much more to protect America...  \n",
       "4  Congress could do much more to protect America...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.join(df_articles.set_index(\"uniqueID\"), on=\"articleID\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(iterable, batch_size):\n",
    "    \"\"\"Splits an iterable into smaller batches.\"\"\"\n",
    "    iterable = iter(iterable)\n",
    "    while batch := list(islice(iterable, batch_size)):\n",
    "        yield batch\n",
    "\n",
    "def save_to_csv(path, prompts, responses, temperature, top_p, top_k):\n",
    "    \"\"\"Saves prompts, responses and sampling parameters to a CSV file.\"\"\"\n",
    "    with open(path, mode='a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        for prompt, response in zip(prompts, responses):\n",
    "            writer.writerow([prompt, response, temperature, top_p, top_k])\n",
    "\n",
    "def generate_responses(model, prompts, sampling_params):\n",
    "    \"\"\"Generate a batch of outputs using vLLM with customizable sampling parameters.\"\"\"\n",
    "    outputs = model.chat(prompts, sampling_params=sampling_params, use_tqdm=False)\n",
    "    \n",
    "    return [sample.outputs[0].text.replace('\"', '') for sample in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PROMPT = [{\"role\": \"system\", \"content\": \"You are a helpful asistant for writing comments based on article abstracts. Based on provided article abstract and a sample comment generate similar comment related to the article. MAKE SURE TO REPLAY ONLY WITH THE COMMENT.\"},\n",
    "                {\"role\": \"user\", \"content\": \"Abstract: \\n {abstract} \\n  Comment: \\n {comment}.\"},\n",
    "                {\"role\": \"assistant\", \"content\": \"Similar comment: \\n\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    [\n",
    "        BASE_PROMPT[0],  # The system message\n",
    "        {\"role\": \"user\", \"content\": BASE_PROMPT[1][\"content\"].format(abstract=abstract, comment=comment)},  # Formatted user message\n",
    "        BASE_PROMPT[2]  # The assistant message\n",
    "    ]\n",
    "    for abstract, comment in df[[\"abstract\", \"commentBody\"]].values\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/19478 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19479it [37:49,  8.58it/s]                             \n"
     ]
    }
   ],
   "source": [
    "lens = []\n",
    "batch_size = 256\n",
    "for prompts_batch in tqdm(batchify(prompts, batch_size), total=len(prompts) // batch_size):\n",
    "    tokens = tokenizer.apply_chat_template(prompts_batch)\n",
    "    lens.extend([len(token) for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "too_large = [i for i, l in enumerate(lens) if l > 32_768]\n",
    "too_large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"articleID\", \"abstract\"], inplace=True)\n",
    "df.to_csv(\"../../data/data_human/nyt_comments.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = [\n",
    "    SamplingParams(temperature=0.0, top_p=1.0, top_k=-1, max_tokens=40_000, seed=SEED),  # Pure Greedy (fully deterministic)\n",
    "    SamplingParams(temperature=0.2, top_p=1.0, top_k=-1, max_tokens=40_000, seed=SEED),  # Highly Deterministic\n",
    "    SamplingParams(temperature=0.5, top_p=0.95, top_k=100, max_tokens=40_000, seed=SEED), # Mildly Deterministic but Flexible\n",
    "    SamplingParams(temperature=0.7, top_p=0.9, top_k=50, max_tokens=40_000, seed=SEED),  # Balanced and Natural\n",
    "    SamplingParams(temperature=0.9, top_p=0.8, top_k=40, max_tokens=40_000, seed=SEED),  # Slightly More Diverse but Coherent\n",
    "    SamplingParams(temperature=1.0, top_p=0.95, top_k=30, max_tokens=40_000, seed=SEED), # Default Creative Mode\n",
    "    SamplingParams(temperature=1.2, top_p=0.7, top_k=20, max_tokens=40_000, seed=SEED),  # Highly Creative\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "llms = [\"meta-llama/Llama-3.2-1B-Instruct\"]\n",
    "batch_size = 8\n",
    "base_path = \"../../data/data_ai/nyt_comments/nyt-comments_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 02-15 05:11:44 config.py:2386] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 02-15 05:12:05 config.py:542] This model supports multiple tasks: {'embed', 'generate', 'score', 'reward', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 02-15 05:12:05 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='meta-llama/Llama-3.2-1B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-1B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=10000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-3.2-1B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "WARNING 02-15 05:13:23 interface.py:284] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "INFO 02-15 05:13:23 cuda.py:179] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 02-15 05:13:23 cuda.py:227] Using XFormers backend.\n",
      "INFO 02-15 05:13:28 model_runner.py:1110] Starting to load model meta-llama/Llama-3.2-1B-Instruct...\n",
      "INFO 02-15 05:13:31 weight_utils.py:252] Using model weights format ['*.safetensors']\n",
      "INFO 02-15 05:13:32 weight_utils.py:297] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c564b5831d342bd8bfd0983e29a5fbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-15 05:14:19 model_runner.py:1115] Loading model weights took 2.3185 GB\n",
      "INFO 02-15 05:14:32 worker.py:267] Memory profiling takes 11.18 seconds\n",
      "INFO 02-15 05:14:32 worker.py:267] the current vLLM instance can use total_gpu_memory (6.00GiB) x gpu_memory_utilization (0.90) = 5.40GiB\n",
      "INFO 02-15 05:14:32 worker.py:267] model weights take 2.32GiB; non_torch_memory takes 0.04GiB; PyTorch activation peak memory takes 1.21GiB; the rest of the memory reserved for KV Cache is 1.84GiB.\n",
      "INFO 02-15 05:14:33 executor_base.py:110] # CUDA blocks: 3761, # CPU blocks: 8192\n",
      "INFO 02-15 05:14:33 executor_base.py:115] Maximum concurrency for 10000 tokens per request: 6.02x\n",
      "INFO 02-15 05:15:16 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [01:55<00:00,  3.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-15 05:17:12 model_runner.py:1562] Graph capturing finished in 116 secs, took 0.12 GiB\n",
      "INFO 02-15 05:17:12 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 172.83 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/623307 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-15 05:17:12 chat_utils.py:332] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/623307 [00:17<1539:29:42,  8.89s/it]\n"
     ]
    }
   ],
   "source": [
    "for llm in llms:\n",
    "    model = LLM(model=llm, dtype=\"half\", max_model_len = 10_000)\n",
    "    csv_path = f\"{base_path}{llm.split('/')[-1]}.csv\"\n",
    "\n",
    "\n",
    "    # init csv file\n",
    "    with open(csv_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"prompt\", \"response\", \"temperature\", \"top_p\", \"top_k\"])\n",
    "\n",
    "    cnt = 0\n",
    "    for prompts_batch in tqdm(batchify(prompts, batch_size), total=len(prompts) // batch_size):\n",
    "        params = random.choice(sampling_params)\n",
    "        responses = generate_responses(model, prompts_batch, params)\n",
    "        save_to_csv(csv_path, prompts_batch, responses, params.temperature, params.top_p, params.top_k)\n",
    "        cnt += 1\n",
    "        if cnt > 2:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>response</th>\n",
       "      <th>temperature</th>\n",
       "      <th>top_p</th>\n",
       "      <th>top_k</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'role': 'system', 'content': 'You are a help...</td>\n",
       "      <td>It's essential to note that the VA home loan p...</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.7</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{'role': 'system', 'content': 'You are a help...</td>\n",
       "      <td>@VAloanExpert  I'm a huge fan of the VA loan ...</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.7</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[{'role': 'system', 'content': 'You are a help...</td>\n",
       "      <td>@expatlife23\\n\\nIt's not necessarily about ta...</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.7</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[{'role': 'system', 'content': 'You are a help...</td>\n",
       "      <td>I'm so frustrated with the lack of protection ...</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.7</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[{'role': 'system', 'content': 'You are a help...</td>\n",
       "      <td>@VAloanexpert, I'm glad you're doing your res...</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.7</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0  [{'role': 'system', 'content': 'You are a help...   \n",
       "1  [{'role': 'system', 'content': 'You are a help...   \n",
       "2  [{'role': 'system', 'content': 'You are a help...   \n",
       "3  [{'role': 'system', 'content': 'You are a help...   \n",
       "4  [{'role': 'system', 'content': 'You are a help...   \n",
       "\n",
       "                                            response  temperature  top_p  \\\n",
       "0  It's essential to note that the VA home loan p...          1.2    0.7   \n",
       "1   @VAloanExpert  I'm a huge fan of the VA loan ...          1.2    0.7   \n",
       "2   @expatlife23\\n\\nIt's not necessarily about ta...          1.2    0.7   \n",
       "3  I'm so frustrated with the lack of protection ...          1.2    0.7   \n",
       "4   @VAloanexpert, I'm glad you're doing your res...          1.2    0.7   \n",
       "\n",
       "   top_k  \n",
       "0     20  \n",
       "1     20  \n",
       "2     20  \n",
       "3     20  \n",
       "4     20  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../../data/data_ai/nyt_comments/nyt-comments_Llama-3.2-1B-Instruct.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
