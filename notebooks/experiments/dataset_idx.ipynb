{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_csv_paths(folder_path, recursive=False):\n",
    "    if recursive:\n",
    "        # Walk through all subdirectories\n",
    "        file_paths = [os.path.join(root, file) \n",
    "                      for root, _, files in os.walk(folder_path) \n",
    "                      for file in files if file.endswith('.csv')]\n",
    "    else:\n",
    "        # Get files in the root folder only\n",
    "        file_paths = [os.path.join(folder_path, file) \n",
    "                      for file in os.listdir(folder_path) \n",
    "                      if file.endswith('.csv')]\n",
    "    \n",
    "    return file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>model</th>\n",
       "      <th>num_samples</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_chars</th>\n",
       "      <th>num_tokens</th>\n",
       "      <th>avg_token_per_sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nyt-comments</td>\n",
       "      <td>human</td>\n",
       "      <td>4223213</td>\n",
       "      <td>18713462</td>\n",
       "      <td>75699056</td>\n",
       "      <td>1418028952</td>\n",
       "      <td>367081295</td>\n",
       "      <td>86.919910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blogs</td>\n",
       "      <td>human</td>\n",
       "      <td>576731</td>\n",
       "      <td>8328335</td>\n",
       "      <td>11967700</td>\n",
       "      <td>557323671</td>\n",
       "      <td>164358740</td>\n",
       "      <td>284.983363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>raid</td>\n",
       "      <td>human</td>\n",
       "      <td>138244</td>\n",
       "      <td>1808789</td>\n",
       "      <td>7756169</td>\n",
       "      <td>215270586</td>\n",
       "      <td>95663743</td>\n",
       "      <td>691.992007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>writingprompts</td>\n",
       "      <td>human</td>\n",
       "      <td>303140</td>\n",
       "      <td>13802625</td>\n",
       "      <td>4407470</td>\n",
       "      <td>721933659</td>\n",
       "      <td>209316368</td>\n",
       "      <td>690.494056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>essays</td>\n",
       "      <td>human</td>\n",
       "      <td>2638</td>\n",
       "      <td>123010</td>\n",
       "      <td>67709</td>\n",
       "      <td>6702698</td>\n",
       "      <td>1910966</td>\n",
       "      <td>724.399545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>blogs</td>\n",
       "      <td>Phi-3-medium-128k-instruct</td>\n",
       "      <td>28836</td>\n",
       "      <td>928790</td>\n",
       "      <td>2403937</td>\n",
       "      <td>70950486</td>\n",
       "      <td>18821223</td>\n",
       "      <td>652.698814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>blogs</td>\n",
       "      <td>Qwen2-7B-Instruct</td>\n",
       "      <td>28836</td>\n",
       "      <td>447564</td>\n",
       "      <td>759965</td>\n",
       "      <td>41207063</td>\n",
       "      <td>10143808</td>\n",
       "      <td>351.775836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>blogs</td>\n",
       "      <td>Qwen2-72B-Instruct-AWQ</td>\n",
       "      <td>28835</td>\n",
       "      <td>590847</td>\n",
       "      <td>561720</td>\n",
       "      <td>45952728</td>\n",
       "      <td>11875452</td>\n",
       "      <td>411.841581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>blogs</td>\n",
       "      <td>Llama-3.1-8B-Instruct</td>\n",
       "      <td>28836</td>\n",
       "      <td>471158</td>\n",
       "      <td>895753</td>\n",
       "      <td>43924826</td>\n",
       "      <td>11125419</td>\n",
       "      <td>385.817000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>blogs</td>\n",
       "      <td>Meta-Llama-3.1-70B-Instruct-AWQ-INT4</td>\n",
       "      <td>28836</td>\n",
       "      <td>363638</td>\n",
       "      <td>718404</td>\n",
       "      <td>32356938</td>\n",
       "      <td>8184704</td>\n",
       "      <td>283.836316</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>189 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               data                                 model  num_samples  \\\n",
       "0      nyt-comments                                 human      4223213   \n",
       "1             blogs                                 human       576731   \n",
       "2              raid                                 human       138244   \n",
       "4    writingprompts                                 human       303140   \n",
       "5            essays                                 human         2638   \n",
       "..              ...                                   ...          ...   \n",
       "190           blogs            Phi-3-medium-128k-instruct        28836   \n",
       "191           blogs                     Qwen2-7B-Instruct        28836   \n",
       "192           blogs                Qwen2-72B-Instruct-AWQ        28835   \n",
       "193           blogs                 Llama-3.1-8B-Instruct        28836   \n",
       "194           blogs  Meta-Llama-3.1-70B-Instruct-AWQ-INT4        28836   \n",
       "\n",
       "     num_sentences  num_words   num_chars  num_tokens  avg_token_per_sample  \n",
       "0         18713462   75699056  1418028952   367081295             86.919910  \n",
       "1          8328335   11967700   557323671   164358740            284.983363  \n",
       "2          1808789    7756169   215270586    95663743            691.992007  \n",
       "4         13802625    4407470   721933659   209316368            690.494056  \n",
       "5           123010      67709     6702698     1910966            724.399545  \n",
       "..             ...        ...         ...         ...                   ...  \n",
       "190         928790    2403937    70950486    18821223            652.698814  \n",
       "191         447564     759965    41207063    10143808            351.775836  \n",
       "192         590847     561720    45952728    11875452            411.841581  \n",
       "193         471158     895753    43924826    11125419            385.817000  \n",
       "194         363638     718404    32356938     8184704            283.836316  \n",
       "\n",
       "[189 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_main = pd.read_csv('../../data/stats/data_stats_master.csv')\n",
    "df_main = df_main[df_main[\"data\"] != \"natural-questions\"]\n",
    "df_main = df_main[df_main[\"model\"] != \"gpt-4.1-nano-2025-04-14\"]\n",
    "df_main[\"avg_token_per_sample\"] = df_main[\"num_tokens\"] / df_main[\"num_samples\"]\n",
    "df_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_HUMAN_PATH = \"../../data/stats/data_human\"\n",
    "DATA_AI_PATH = \"../../data/stats/data_ai\"\n",
    "DATASET_IDX_PATH = \"../../data/datasets/test3_idx.csv\"\n",
    "paths = get_csv_paths(DATA_HUMAN_PATH) + get_csv_paths(DATA_AI_PATH, recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = dict({f\"{path.split(\"/\")[-1].split(\"_\")[0]}_{path.split(\"/\")[-1].split(\"_\")[1]}\": pd.read_csv(path) for path in paths})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 5e7\n",
    "total_tokens = 0\n",
    "total_sentences = 0\n",
    "total_samples = 0\n",
    "batch_size = 16\n",
    "cols_c0 = [\"human\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds in [\"blogs\", \"essays\"]:\n",
    "    df_main.loc[df_main[\"data\"].values == ds, \"prob\"] =  (1 / df_main.loc[df_main[\"data\"].values == ds, \"avg_token_per_sample\"].values) /  (1 / df_main.loc[df_main[\"data\"].values == ds, \"avg_token_per_sample\"]).sum()\n",
    "    mask_c0 = (df_main[\"data\"].values == ds) & (df_main[\"model\"].isin(cols_c0))\n",
    "    mask_c1 = (df_main[\"data\"].values == ds) & (~df_main[\"model\"].isin(cols_c0))\n",
    "\n",
    "    class0 = df_main[mask_c0]\n",
    "    class1 = df_main[mask_c1]\n",
    "\n",
    "    s1 = (class0[\"avg_token_per_sample\"] * class0[\"prob\"]).sum()\n",
    "    s2 = (class1[\"avg_token_per_sample\"] * class1[\"prob\"]).sum()\n",
    "    p1 = class0[\"prob\"].sum()\n",
    "    p2 = class1[\"prob\"].sum()\n",
    "\n",
    "    c1 = 1 / (s2 / s1 * p1 + p2)\n",
    "    c0 = c1 * s2 / s1\n",
    "\n",
    "    df_main.loc[mask_c0, \"prob\"] *= c0\n",
    "    df_main.loc[mask_c1, \"prob\"] *= c1\n",
    "\n",
    "weights = [\n",
    "    (df_main.loc[df_main[\"data\"] == ds, \"num_tokens\"] * df_main.loc[df_main[\"data\"] == ds, \"prob\"]).sum()\n",
    "    for ds in df_main[\"data\"].unique()\n",
    "]\n",
    "probs = np.array(weights) / np.sum(weights)\n",
    "\n",
    "# total_tokens = 0\n",
    "# total_sentences = 0\n",
    "# total_samples = 0\n",
    "# cnt = 0\n",
    "# while total_tokens < max_tokens:\n",
    "#     data = np.random.choice(df_main[\"data\"].unique(), p=probs)\n",
    "#     tmp = df_main[(df_main[\"data\"] == data)]\n",
    "#     model = np.random.choice(tmp[\"model\"], p=tmp[\"prob\"])\n",
    "\n",
    "#     stat = stats[f\"{data}_{model}\"]\n",
    "\n",
    "#     slct = stat.sample(n=batch_size)\n",
    "#     stat.drop(slct.index, inplace=True)\n",
    "\n",
    "#     total_tokens += slct.sum()[\"num_tokens\"]\n",
    "#     total_sentences += slct.sum()[\"num_sentences\"]\n",
    "#     total_samples += batch_size\n",
    "\n",
    "\n",
    "#     # save data, model, slct.index to csv\n",
    "#     slct[\"data\"] = data\n",
    "#     slct[\"model\"] = model\n",
    "#     slct.reset_index(inplace=True)\n",
    "#     # slct.drop(columns=[\"num_sentences\", \"num_words\", \"num_chars\", \"num_tokens\"], inplace=True)\n",
    "#     slct.to_csv(DATASET_IDX_PATH, mode='a', header=not os.path.exists(DATASET_IDX_PATH), index=False)\n",
    "\n",
    "#     cnt += 1\n",
    "#     if cnt % 1000 == 0:\n",
    "#         print(f\"total_tokens: {total_tokens}, total_sentences: {total_sentences}, total_samples: {total_samples}\")\n",
    "\n",
    "# print(\n",
    "#     f\"Final samples: {total_samples}, Final sentences: {total_sentences}, Final tokens: {total_tokens}\"\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_chars</th>\n",
       "      <th>num_tokens</th>\n",
       "      <th>data</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22550</td>\n",
       "      <td>34</td>\n",
       "      <td>20</td>\n",
       "      <td>2522</td>\n",
       "      <td>636</td>\n",
       "      <td>blogs</td>\n",
       "      <td>Llama-3.1-8B-Instruct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9473</td>\n",
       "      <td>15</td>\n",
       "      <td>26</td>\n",
       "      <td>1787</td>\n",
       "      <td>405</td>\n",
       "      <td>blogs</td>\n",
       "      <td>Llama-3.1-8B-Instruct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20600</td>\n",
       "      <td>11</td>\n",
       "      <td>26</td>\n",
       "      <td>1480</td>\n",
       "      <td>323</td>\n",
       "      <td>blogs</td>\n",
       "      <td>Llama-3.1-8B-Instruct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23943</td>\n",
       "      <td>6</td>\n",
       "      <td>78</td>\n",
       "      <td>1058</td>\n",
       "      <td>244</td>\n",
       "      <td>blogs</td>\n",
       "      <td>Llama-3.1-8B-Instruct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20857</td>\n",
       "      <td>22</td>\n",
       "      <td>24</td>\n",
       "      <td>1790</td>\n",
       "      <td>478</td>\n",
       "      <td>blogs</td>\n",
       "      <td>Llama-3.1-8B-Instruct</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  num_sentences  num_words  num_chars  num_tokens   data  \\\n",
       "0  22550             34         20       2522         636  blogs   \n",
       "1   9473             15         26       1787         405  blogs   \n",
       "2  20600             11         26       1480         323  blogs   \n",
       "3  23943              6         78       1058         244  blogs   \n",
       "4  20857             22         24       1790         478  blogs   \n",
       "\n",
       "                   model  \n",
       "0  Llama-3.1-8B-Instruct  \n",
       "1  Llama-3.1-8B-Instruct  \n",
       "2  Llama-3.1-8B-Instruct  \n",
       "3  Llama-3.1-8B-Instruct  \n",
       "4  Llama-3.1-8B-Instruct  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(DATASET_IDX_PATH)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"is_human\"] = np.where(df[\"model\"].isin(cols_c0), \"human\", \"ai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_chars</th>\n",
       "      <th>num_tokens</th>\n",
       "      <th>data</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_human</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ai</th>\n",
       "      <td>989425257</td>\n",
       "      <td>1245943</td>\n",
       "      <td>1864094</td>\n",
       "      <td>91922494</td>\n",
       "      <td>24886382</td>\n",
       "      <td>blogsblogsblogsblogsblogsblogsblogsblogsblogsb...</td>\n",
       "      <td>Llama-3.1-8B-InstructLlama-3.1-8B-InstructLlam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>human</th>\n",
       "      <td>24154483641</td>\n",
       "      <td>1286201</td>\n",
       "      <td>1777485</td>\n",
       "      <td>85321878</td>\n",
       "      <td>25115887</td>\n",
       "      <td>blogsblogsblogsblogsblogsblogsblogsblogsblogsb...</td>\n",
       "      <td>humanhumanhumanhumanhumanhumanhumanhumanhumanh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                index  num_sentences  num_words  num_chars  num_tokens  \\\n",
       "is_human                                                                 \n",
       "ai          989425257        1245943    1864094   91922494    24886382   \n",
       "human     24154483641        1286201    1777485   85321878    25115887   \n",
       "\n",
       "                                                       data  \\\n",
       "is_human                                                      \n",
       "ai        blogsblogsblogsblogsblogsblogsblogsblogsblogsb...   \n",
       "human     blogsblogsblogsblogsblogsblogsblogsblogsblogsb...   \n",
       "\n",
       "                                                      model  \n",
       "is_human                                                     \n",
       "ai        Llama-3.1-8B-InstructLlama-3.1-8B-InstructLlam...  \n",
       "human     humanhumanhumanhumanhumanhumanhumanhumanhumanh...  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"is_human\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_chars</th>\n",
       "      <th>num_tokens</th>\n",
       "      <th>data</th>\n",
       "      <th>is_human</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Falcon3-3B-Instruct</th>\n",
       "      <td>40449566</td>\n",
       "      <td>56955</td>\n",
       "      <td>43858</td>\n",
       "      <td>4635235</td>\n",
       "      <td>1226797</td>\n",
       "      <td>blogsblogsblogsblogsblogsblogsblogsblogsblogsb...</td>\n",
       "      <td>aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Falcon3-7B-Instruct</th>\n",
       "      <td>68720793</td>\n",
       "      <td>66238</td>\n",
       "      <td>89868</td>\n",
       "      <td>4854747</td>\n",
       "      <td>1283953</td>\n",
       "      <td>blogsblogsblogsblogsblogsblogsblogsblogsblogsb...</td>\n",
       "      <td>aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Llama-3.1-8B-Instruct</th>\n",
       "      <td>41411787</td>\n",
       "      <td>48073</td>\n",
       "      <td>89742</td>\n",
       "      <td>4495386</td>\n",
       "      <td>1152977</td>\n",
       "      <td>blogsblogsblogsblogsblogsblogsblogsblogsblogsb...</td>\n",
       "      <td>aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Llama-3.2-3B-Instruct</th>\n",
       "      <td>43652666</td>\n",
       "      <td>46631</td>\n",
       "      <td>70223</td>\n",
       "      <td>4399048</td>\n",
       "      <td>1108830</td>\n",
       "      <td>blogsblogsblogsblogsblogsblogsblogsblogsblogsb...</td>\n",
       "      <td>aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Meta-Llama-3.1-70B-Instruct-AWQ-INT4</th>\n",
       "      <td>63983411</td>\n",
       "      <td>61399</td>\n",
       "      <td>104453</td>\n",
       "      <td>5374332</td>\n",
       "      <td>1370300</td>\n",
       "      <td>blogsblogsblogsblogsblogsblogsblogsblogsblogsb...</td>\n",
       "      <td>aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Meta-Llama-3.3-70B-Instruct-AWQ-INT4</th>\n",
       "      <td>59456762</td>\n",
       "      <td>53363</td>\n",
       "      <td>94822</td>\n",
       "      <td>4889710</td>\n",
       "      <td>1242738</td>\n",
       "      <td>blogsblogsblogsblogsblogsblogsblogsblogsblogsb...</td>\n",
       "      <td>aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ministral-8B-Instruct-2410</th>\n",
       "      <td>46685710</td>\n",
       "      <td>84620</td>\n",
       "      <td>91459</td>\n",
       "      <td>4686317</td>\n",
       "      <td>1337393</td>\n",
       "      <td>blogsblogsblogsblogsblogsblogsblogsblogsblogsb...</td>\n",
       "      <td>aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mistral-Nemo-Instruct-2407</th>\n",
       "      <td>77235804</td>\n",
       "      <td>72022</td>\n",
       "      <td>141751</td>\n",
       "      <td>4969484</td>\n",
       "      <td>1406950</td>\n",
       "      <td>blogsblogsblogsblogsblogsblogsblogsblogsblogsb...</td>\n",
       "      <td>aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Phi-3-medium-128k-instruct</th>\n",
       "      <td>29849886</td>\n",
       "      <td>65417</td>\n",
       "      <td>128841</td>\n",
       "      <td>5067603</td>\n",
       "      <td>1349274</td>\n",
       "      <td>blogsblogsblogsblogsblogsblogsblogsblogsblogsb...</td>\n",
       "      <td>aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Phi-3-mini-128k-instruct</th>\n",
       "      <td>25448181</td>\n",
       "      <td>90611</td>\n",
       "      <td>214916</td>\n",
       "      <td>5202073</td>\n",
       "      <td>1401653</td>\n",
       "      <td>blogsblogsblogsblogsblogsblogsblogsblogsblogsb...</td>\n",
       "      <td>aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Phi-3-small-128k-instruct</th>\n",
       "      <td>30501319</td>\n",
       "      <td>50908</td>\n",
       "      <td>49944</td>\n",
       "      <td>4429728</td>\n",
       "      <td>1152225</td>\n",
       "      <td>blogsblogsblogsblogsblogsblogsblogsblogsblogsb...</td>\n",
       "      <td>aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Phi-3.5-mini-instruct</th>\n",
       "      <td>16121163</td>\n",
       "      <td>49014</td>\n",
       "      <td>146001</td>\n",
       "      <td>3690680</td>\n",
       "      <td>1128726</td>\n",
       "      <td>blogsblogsblogsblogsblogsblogsblogsblogsblogsb...</td>\n",
       "      <td>aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Phi-4-mini-instruct</th>\n",
       "      <td>33663988</td>\n",
       "      <td>71849</td>\n",
       "      <td>152293</td>\n",
       "      <td>4014292</td>\n",
       "      <td>1338002</td>\n",
       "      <td>blogsblogsblogsblogsblogsblogsblogsblogsblogsb...</td>\n",
       "      <td>aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Qwen2-72B-Instruct-AWQ</th>\n",
       "      <td>38405990</td>\n",
       "      <td>56129</td>\n",
       "      <td>42181</td>\n",
       "      <td>4356626</td>\n",
       "      <td>1130762</td>\n",
       "      <td>blogsblogsblogsblogsblogsblogsblogsblogsblogsb...</td>\n",
       "      <td>aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Qwen2-7B-Instruct</th>\n",
       "      <td>43453256</td>\n",
       "      <td>51986</td>\n",
       "      <td>58331</td>\n",
       "      <td>4791534</td>\n",
       "      <td>1174414</td>\n",
       "      <td>blogsblogsblogsblogsblogsblogsblogsblogsblogsb...</td>\n",
       "      <td>aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Qwen2.5-14B-Instruct</th>\n",
       "      <td>77507021</td>\n",
       "      <td>64178</td>\n",
       "      <td>78686</td>\n",
       "      <td>4662054</td>\n",
       "      <td>1232185</td>\n",
       "      <td>blogsblogsblogsblogsblogsblogsblogsblogsblogsb...</td>\n",
       "      <td>aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Qwen2.5-3B-Instruct</th>\n",
       "      <td>51593904</td>\n",
       "      <td>58719</td>\n",
       "      <td>71521</td>\n",
       "      <td>4026677</td>\n",
       "      <td>1119966</td>\n",
       "      <td>blogsblogsblogsblogsblogsblogsblogsblogsblogsb...</td>\n",
       "      <td>aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Qwen2.5-72B-Instruct-AWQ</th>\n",
       "      <td>63937051</td>\n",
       "      <td>64633</td>\n",
       "      <td>67706</td>\n",
       "      <td>4534268</td>\n",
       "      <td>1267075</td>\n",
       "      <td>blogsblogsblogsblogsblogsblogsblogsblogsblogsb...</td>\n",
       "      <td>aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Qwen2.5-7B-Instruct</th>\n",
       "      <td>72293626</td>\n",
       "      <td>67418</td>\n",
       "      <td>65840</td>\n",
       "      <td>4550287</td>\n",
       "      <td>1266062</td>\n",
       "      <td>blogsblogsblogsblogsblogsblogsblogsblogsblogsb...</td>\n",
       "      <td>aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>human</th>\n",
       "      <td>24154483641</td>\n",
       "      <td>1286201</td>\n",
       "      <td>1777485</td>\n",
       "      <td>85321878</td>\n",
       "      <td>25115887</td>\n",
       "      <td>blogsblogsblogsblogsblogsblogsblogsblogsblogsb...</td>\n",
       "      <td>humanhumanhumanhumanhumanhumanhumanhumanhumanh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phi-4</th>\n",
       "      <td>65053373</td>\n",
       "      <td>65780</td>\n",
       "      <td>61658</td>\n",
       "      <td>4292413</td>\n",
       "      <td>1196100</td>\n",
       "      <td>blogsblogsblogsblogsblogsblogsblogsblogsblogsb...</td>\n",
       "      <td>aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            index  num_sentences  num_words  \\\n",
       "model                                                                         \n",
       "Falcon3-3B-Instruct                      40449566          56955      43858   \n",
       "Falcon3-7B-Instruct                      68720793          66238      89868   \n",
       "Llama-3.1-8B-Instruct                    41411787          48073      89742   \n",
       "Llama-3.2-3B-Instruct                    43652666          46631      70223   \n",
       "Meta-Llama-3.1-70B-Instruct-AWQ-INT4     63983411          61399     104453   \n",
       "Meta-Llama-3.3-70B-Instruct-AWQ-INT4     59456762          53363      94822   \n",
       "Ministral-8B-Instruct-2410               46685710          84620      91459   \n",
       "Mistral-Nemo-Instruct-2407               77235804          72022     141751   \n",
       "Phi-3-medium-128k-instruct               29849886          65417     128841   \n",
       "Phi-3-mini-128k-instruct                 25448181          90611     214916   \n",
       "Phi-3-small-128k-instruct                30501319          50908      49944   \n",
       "Phi-3.5-mini-instruct                    16121163          49014     146001   \n",
       "Phi-4-mini-instruct                      33663988          71849     152293   \n",
       "Qwen2-72B-Instruct-AWQ                   38405990          56129      42181   \n",
       "Qwen2-7B-Instruct                        43453256          51986      58331   \n",
       "Qwen2.5-14B-Instruct                     77507021          64178      78686   \n",
       "Qwen2.5-3B-Instruct                      51593904          58719      71521   \n",
       "Qwen2.5-72B-Instruct-AWQ                 63937051          64633      67706   \n",
       "Qwen2.5-7B-Instruct                      72293626          67418      65840   \n",
       "human                                 24154483641        1286201    1777485   \n",
       "phi-4                                    65053373          65780      61658   \n",
       "\n",
       "                                      num_chars  num_tokens  \\\n",
       "model                                                         \n",
       "Falcon3-3B-Instruct                     4635235     1226797   \n",
       "Falcon3-7B-Instruct                     4854747     1283953   \n",
       "Llama-3.1-8B-Instruct                   4495386     1152977   \n",
       "Llama-3.2-3B-Instruct                   4399048     1108830   \n",
       "Meta-Llama-3.1-70B-Instruct-AWQ-INT4    5374332     1370300   \n",
       "Meta-Llama-3.3-70B-Instruct-AWQ-INT4    4889710     1242738   \n",
       "Ministral-8B-Instruct-2410              4686317     1337393   \n",
       "Mistral-Nemo-Instruct-2407              4969484     1406950   \n",
       "Phi-3-medium-128k-instruct              5067603     1349274   \n",
       "Phi-3-mini-128k-instruct                5202073     1401653   \n",
       "Phi-3-small-128k-instruct               4429728     1152225   \n",
       "Phi-3.5-mini-instruct                   3690680     1128726   \n",
       "Phi-4-mini-instruct                     4014292     1338002   \n",
       "Qwen2-72B-Instruct-AWQ                  4356626     1130762   \n",
       "Qwen2-7B-Instruct                       4791534     1174414   \n",
       "Qwen2.5-14B-Instruct                    4662054     1232185   \n",
       "Qwen2.5-3B-Instruct                     4026677     1119966   \n",
       "Qwen2.5-72B-Instruct-AWQ                4534268     1267075   \n",
       "Qwen2.5-7B-Instruct                     4550287     1266062   \n",
       "human                                  85321878    25115887   \n",
       "phi-4                                   4292413     1196100   \n",
       "\n",
       "                                                                                   data  \\\n",
       "model                                                                                     \n",
       "Falcon3-3B-Instruct                   blogsblogsblogsblogsblogsblogsblogsblogsblogsb...   \n",
       "Falcon3-7B-Instruct                   blogsblogsblogsblogsblogsblogsblogsblogsblogsb...   \n",
       "Llama-3.1-8B-Instruct                 blogsblogsblogsblogsblogsblogsblogsblogsblogsb...   \n",
       "Llama-3.2-3B-Instruct                 blogsblogsblogsblogsblogsblogsblogsblogsblogsb...   \n",
       "Meta-Llama-3.1-70B-Instruct-AWQ-INT4  blogsblogsblogsblogsblogsblogsblogsblogsblogsb...   \n",
       "Meta-Llama-3.3-70B-Instruct-AWQ-INT4  blogsblogsblogsblogsblogsblogsblogsblogsblogsb...   \n",
       "Ministral-8B-Instruct-2410            blogsblogsblogsblogsblogsblogsblogsblogsblogsb...   \n",
       "Mistral-Nemo-Instruct-2407            blogsblogsblogsblogsblogsblogsblogsblogsblogsb...   \n",
       "Phi-3-medium-128k-instruct            blogsblogsblogsblogsblogsblogsblogsblogsblogsb...   \n",
       "Phi-3-mini-128k-instruct              blogsblogsblogsblogsblogsblogsblogsblogsblogsb...   \n",
       "Phi-3-small-128k-instruct             blogsblogsblogsblogsblogsblogsblogsblogsblogsb...   \n",
       "Phi-3.5-mini-instruct                 blogsblogsblogsblogsblogsblogsblogsblogsblogsb...   \n",
       "Phi-4-mini-instruct                   blogsblogsblogsblogsblogsblogsblogsblogsblogsb...   \n",
       "Qwen2-72B-Instruct-AWQ                blogsblogsblogsblogsblogsblogsblogsblogsblogsb...   \n",
       "Qwen2-7B-Instruct                     blogsblogsblogsblogsblogsblogsblogsblogsblogsb...   \n",
       "Qwen2.5-14B-Instruct                  blogsblogsblogsblogsblogsblogsblogsblogsblogsb...   \n",
       "Qwen2.5-3B-Instruct                   blogsblogsblogsblogsblogsblogsblogsblogsblogsb...   \n",
       "Qwen2.5-72B-Instruct-AWQ              blogsblogsblogsblogsblogsblogsblogsblogsblogsb...   \n",
       "Qwen2.5-7B-Instruct                   blogsblogsblogsblogsblogsblogsblogsblogsblogsb...   \n",
       "human                                 blogsblogsblogsblogsblogsblogsblogsblogsblogsb...   \n",
       "phi-4                                 blogsblogsblogsblogsblogsblogsblogsblogsblogsb...   \n",
       "\n",
       "                                                                               is_human  \n",
       "model                                                                                    \n",
       "Falcon3-3B-Instruct                   aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...  \n",
       "Falcon3-7B-Instruct                   aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...  \n",
       "Llama-3.1-8B-Instruct                 aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...  \n",
       "Llama-3.2-3B-Instruct                 aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...  \n",
       "Meta-Llama-3.1-70B-Instruct-AWQ-INT4  aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...  \n",
       "Meta-Llama-3.3-70B-Instruct-AWQ-INT4  aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...  \n",
       "Ministral-8B-Instruct-2410            aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...  \n",
       "Mistral-Nemo-Instruct-2407            aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...  \n",
       "Phi-3-medium-128k-instruct            aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...  \n",
       "Phi-3-mini-128k-instruct              aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...  \n",
       "Phi-3-small-128k-instruct             aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...  \n",
       "Phi-3.5-mini-instruct                 aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...  \n",
       "Phi-4-mini-instruct                   aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...  \n",
       "Qwen2-72B-Instruct-AWQ                aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...  \n",
       "Qwen2-7B-Instruct                     aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...  \n",
       "Qwen2.5-14B-Instruct                  aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...  \n",
       "Qwen2.5-3B-Instruct                   aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...  \n",
       "Qwen2.5-72B-Instruct-AWQ              aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...  \n",
       "Qwen2.5-7B-Instruct                   aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...  \n",
       "human                                 humanhumanhumanhumanhumanhumanhumanhumanhumanh...  \n",
       "phi-4                                 aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby([\"model\"]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
