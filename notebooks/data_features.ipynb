{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/majkel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/majkel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "import spacy\n",
    "import textstat\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.stats import entropy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_features(text):\n",
    "    words = word_tokenize(text)\n",
    "    sentences = sent_tokenize(text)\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    unique_words = set(words)\n",
    "    \n",
    "    features = {\n",
    "        \"word_count\": len(words),\n",
    "        \"character_count\": sum(len(word) for word in words),\n",
    "        \"average_word_length\": sum(len(word) for word in words) / len(words),\n",
    "        \"sentence_count\": len(sentences),\n",
    "        \"unique_words_ratio\": len(unique_words) / len(words),\n",
    "        \"stopword_ratio\": len([word for word in words if word.lower() in stop_words]) / len(words)\n",
    "    }\n",
    "    return features\n",
    "\n",
    "def nlp_features(text):\n",
    "    doc = nlp(text)\n",
    "    pos_counts = Counter([token.pos_ for token in doc])\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    sentiment_scores = [token.sentiment for token in doc if token.sentiment != 0]\n",
    "    \n",
    "    features = {\n",
    "        \"noun_ratio\": pos_counts.get(\"NOUN\", 0) / len(doc),\n",
    "        \"verb_ratio\": pos_counts.get(\"VERB\", 0) / len(doc),\n",
    "        \"adjective_ratio\": pos_counts.get(\"ADJ\", 0) / len(doc),\n",
    "        \"average_sentence_length\": sum(len(sent.text.split()) for sent in doc.sents) / len(list(doc.sents)),\n",
    "        \"entity_count\": len(doc.ents),\n",
    "        \"syntactic_depth\": max([len(list(token.ancestors)) for token in doc] or [0]),\n",
    "        \"dependency_distance\": np.mean([abs(token.head.i - token.i) for token in doc if token.head != token]),\n",
    "        \"entity_count\": len(entities),\n",
    "        \"average_sentiment_score\": np.mean(sentiment_scores) if sentiment_scores else 0,\n",
    "        \"sentiment_variability\": np.std(sentiment_scores) if len(sentiment_scores) > 1 else 0,\n",
    "    }\n",
    "    return features\n",
    "\n",
    "def readability_features(text):\n",
    "    features = {\n",
    "        \"flesch_reading_ease\": textstat.flesch_reading_ease(text),\n",
    "        \"gunning_fog_index\": textstat.gunning_fog(text),\n",
    "        \"smog_index\": textstat.smog_index(text),\n",
    "        \"automated_readability_index\": textstat.automated_readability_index(text),\n",
    "        \"dale_chall_readability\": textstat.dale_chall_readability_score(text),\n",
    "    }\n",
    "    return features\n",
    "\n",
    "def stylometric_features(text):\n",
    "    words = word_tokenize(text)\n",
    "    bigrams = list(nltk.bigrams(words))\n",
    "    trigrams = list(nltk.trigrams(words))\n",
    "    \n",
    "    features = {\n",
    "        \"bigram_count\": len(bigrams),\n",
    "        \"trigram_count\": len(trigrams),\n",
    "        \"punctuation_count\": sum(1 for char in text if char in \".,;!?\"),\n",
    "        \"entropy_score\": entropy(list(Counter(words).values())),\n",
    "    }\n",
    "    return features\n",
    "\n",
    "def discourse_features(text):\n",
    "    discourse_markers = {\"however\", \"therefore\", \"moreover\", \"nevertheless\", \"thus\", \"on the other hand\"}\n",
    "    words = word_tokenize(text)\n",
    "    marker_count = sum(1 for word in words if word.lower() in discourse_markers)\n",
    "    \n",
    "    features = {\n",
    "        \"discourse_marker_count\": marker_count,\n",
    "        \"discourse_marker_ratio\": marker_count / len(words) if len(words) > 0 else 0,\n",
    "    }\n",
    "    return features\n",
    "\n",
    "def extract_features_single_text(text):\n",
    "    features = {}\n",
    "    features.update(lexical_features(text))\n",
    "    features.update(nlp_features(text))\n",
    "    features.update(readability_features(text))\n",
    "    features.update(stylometric_features(text))\n",
    "    features.update(discourse_features(text))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_features(texts):\n",
    "    results = []\n",
    "    for text in tqdm(texts):\n",
    "        features = extract_features_single_text(text)\n",
    "        results.append(features)\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_csv_paths(folder_path, recursive=False):\n",
    "    if recursive:\n",
    "        # Walk through all subdirectories\n",
    "        file_paths = [os.path.join(root, file) \n",
    "                      for root, _, files in os.walk(folder_path) \n",
    "                      for file in files if file.endswith('.csv')]\n",
    "    else:\n",
    "        # Get files in the root folder only\n",
    "        file_paths = [os.path.join(folder_path, file) \n",
    "                      for file in os.listdir(folder_path) \n",
    "                      if file.endswith('.csv')]\n",
    "    \n",
    "    return file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_feature_stats(df, stats, data_path, save_path):\n",
    "    df_stat = df.agg(stats).reset_index()\n",
    "\n",
    "    data_name, model = data_path.split(\"/\")[-1].split(\"_\")\n",
    "    model = model.removesuffix(\".csv\")\n",
    "\n",
    "    df_stat[\"model\"] = model\n",
    "    df_stat[\"data\"] = data_name\n",
    "    df_stat.rename(columns={\"index\": \"stat\"}, inplace=True)\n",
    "    df_stat.to_csv(save_path, mode=\"a\", index=False, header=not pd.io.common.file_exists(save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentile(n):\n",
    "    def percentile_(x):\n",
    "        return x.quantile(n)\n",
    "    percentile_.__name__ = 'percentile_{:02.0f}'.format(n*100)\n",
    "    return percentile_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_HUMAN_PATH = \"../data/data_human\"\n",
    "DATA_AI_PATH = \"../data/data_ai\"\n",
    "FEATURES_PATH = \"../data/features/\"\n",
    "FEATURES_STATS_PATH = \"../data/features/features_stats_master.csv\"\n",
    "STATS = ['mean', 'std', 'min', 'max', 'median', 'skew', 'kurtosis', 'var', percentile(0.1), percentile(0.2), percentile(0.3), percentile(0.4), percentile(0.5), percentile(0.6), percentile(0.7), percentile(0.8), percentile(0.9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = get_csv_paths(DATA_HUMAN_PATH) + get_csv_paths(DATA_AI_PATH, recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 356/3000 [00:03<00:29, 89.85it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(path)\n\u001b[1;32m      8\u001b[0m texts \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues[:\u001b[38;5;241m3000\u001b[39m]\n\u001b[0;32m----> 9\u001b[0m df_features \u001b[38;5;241m=\u001b[39m \u001b[43mcalc_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m df_features\u001b[38;5;241m.\u001b[39mto_csv(features_path, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     12\u001b[0m save_feature_stats(df_features, STATS, path, FEATURES_STATS_PATH)\n",
      "Cell \u001b[0;32mIn[47], line 4\u001b[0m, in \u001b[0;36mcalc_features\u001b[0;34m(texts)\u001b[0m\n\u001b[1;32m      2\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m tqdm(texts):\n\u001b[0;32m----> 4\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[43mextract_features_single_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(features)\n\u001b[1;32m      7\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(results)\n",
      "Cell \u001b[0;32mIn[46], line 76\u001b[0m, in \u001b[0;36mextract_features_single_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m#features.update(syntactic_features(text))\u001b[39;00m\n\u001b[1;32m     75\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate(readability_features(text))\n\u001b[0;32m---> 76\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate(\u001b[43mstylometric_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     77\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate(discourse_features(text))\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m features\n",
      "Cell \u001b[0;32mIn[46], line 48\u001b[0m, in \u001b[0;36mstylometric_features\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstylometric_features\u001b[39m(text):\n\u001b[0;32m---> 48\u001b[0m     words \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     bigrams \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(nltk\u001b[38;5;241m.\u001b[39mbigrams(words))\n\u001b[1;32m     50\u001b[0m     trigrams \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(nltk\u001b[38;5;241m.\u001b[39mtrigrams(words))\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/nltk/tokenize/__init__.py:144\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03mReturn a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03musing NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m:type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    142\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m--> 144\u001b[0m     token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_treebank_word_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m ]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/nltk/tokenize/destructive.py:179\u001b[0m, in \u001b[0;36mNLTKWordTokenizer.tokenize\u001b[0;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[1;32m    176\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m text \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m regexp, substitution \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mENDING_QUOTES:\n\u001b[0;32m--> 179\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mregexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubstitution\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m regexp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCONTRACTIONS2:\n\u001b[1;32m    182\u001b[0m     text \u001b[38;5;241m=\u001b[39m regexp\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m1 \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m2 \u001b[39m\u001b[38;5;124m\"\u001b[39m, text)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for path in paths:\n",
    "    if path.split(\"_\")[-1] == \"human.csv\":\n",
    "        features_path = os.path.join(FEATURES_PATH, path.split(\"/\")[-2], path.split(\"/\")[-1].replace(\".csv\", \"_features.csv\"))\n",
    "    else:\n",
    "        features_path = os.path.join(FEATURES_PATH, path.split(\"/\")[-3], path.split(\"/\")[-2], path.split(\"/\")[-1].replace(\".csv\", \"_features.csv\"))\n",
    "\n",
    "    df = pd.read_csv(path)\n",
    "    texts = df[\"text\"].values[:3000]\n",
    "    df_features = calc_features(texts)\n",
    "    df_features.to_csv(features_path, index=False)\n",
    "\n",
    "    save_feature_stats(df_features, STATS, path, FEATURES_STATS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
