{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/majkel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/majkel/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to /home/majkel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import statistics\n",
    "from collections import Counter\n",
    "from typing import List, Union\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import textstat\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from scipy.stats import entropy\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# Load SpaCy model for syntactic features\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Prepare GPT-2 model/tokenizer for perplexity if available\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "_ppl_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "_ppl_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "_ppl_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_metric(string):\n",
    "    string_list = string.split()\n",
    "    counts = np.unique(string_list, return_counts=True)[1]\n",
    "    numerator = np.sum(counts*(counts-1))\n",
    "    n = len(string_list)\n",
    "    denominator = n*(n-1)\n",
    "    return numerator/denominator\n",
    "\n",
    "def lexical_features(text: str) -> dict:\n",
    "    words = word_tokenize(text)\n",
    "    sentences = sent_tokenize(text)\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    unique_words = set(words)\n",
    "    return {\n",
    "        \"word_count\": len(words),\n",
    "        \"character_count\": sum(len(w) for w in words),\n",
    "        \"average_word_length\": sum(len(w) for w in words) / len(words) if words else 0,\n",
    "        \"sentence_count\": len(sentences),\n",
    "        \"TTR\": len(unique_words) / len(words) if words else 0,\n",
    "        \"RTTR\": np.sqrt(len(unique_words)) / len(words) if words else 0,\n",
    "        \"CTTR\": len(unique_words) / ((len(words)*2) ** 0.5) if words else 0,\n",
    "        \"DMetric\": d_metric(text),\n",
    "        \"Mass\": (np.log10(len(words)) - np.log10(len(unique_words))) / (np.log10(len(words))**2),\n",
    "        \"stopword_ratio\": len([w for w in words if w.lower() in stop_words]) / len(words) if words else 0,\n",
    "    }\n",
    "\n",
    "\n",
    "def nlp_features(text: str) -> dict:\n",
    "    doc = nlp(text)\n",
    "    pos_counts = Counter(token.pos_ for token in doc)\n",
    "    entities = list(doc.ents)\n",
    "    sentiment_scores = [token.sentiment for token in doc if token.sentiment != 0]\n",
    "    return {\n",
    "        \"noun_ratio\": pos_counts.get(\"NOUN\", 0) / len(doc) if doc else 0,\n",
    "        \"verb_ratio\": pos_counts.get(\"VERB\", 0) / len(doc) if doc else 0,\n",
    "        \"adjective_ratio\": pos_counts.get(\"ADJ\", 0) / len(doc) if doc else 0,\n",
    "        \"average_sentence_length\": sum(len(sent.text.split()) for sent in doc.sents) / len(list(doc.sents)) if list(doc.sents) else 0,\n",
    "        \"std_sentence_length\": statistics.pstdev([len(sent.text.split()) for sent in doc.sents]) if list(doc.sents) else 0,\n",
    "        \"entity_count\": len(entities),\n",
    "        \"syntactic_depth\": max((len(list(token.ancestors)) for token in doc), default=0),\n",
    "        \"dependency_distance\": np.mean([abs(token.head.i - token.i) for token in doc if token.head != token]) if doc else 0,\n",
    "        \"average_sentiment_score\": np.mean(sentiment_scores) if sentiment_scores else 0,\n",
    "        \"sentiment_variability\": np.std(sentiment_scores) if len(sentiment_scores) > 1 else 0,\n",
    "    }\n",
    "\n",
    "\n",
    "def readability_features(text: str) -> dict:\n",
    "    return {\n",
    "        \"flesch_reading_ease\": textstat.flesch_reading_ease(text),\n",
    "        \"gunning_fog_index\": textstat.gunning_fog(text),\n",
    "        \"smog_index\": textstat.smog_index(text),\n",
    "        \"automated_readability_index\": textstat.automated_readability_index(text),\n",
    "        \"dale_chall_readability\": textstat.dale_chall_readability_score(text),\n",
    "    }\n",
    "\n",
    "\n",
    "def stylometric_features(text: str) -> dict:\n",
    "    words = word_tokenize(text.lower())\n",
    "    return {\n",
    "        \"punctuation_count\": sum(1 for char in text if char in \".,;!?\"),\n",
    "        \"entropy_score\": entropy(list(Counter(words).values())),\n",
    "    }\n",
    "\n",
    "\n",
    "def discourse_features(text: str) -> dict:\n",
    "    markers = {\"however\", \"therefore\", \"moreover\", \"nevertheless\", \"thus\", \"on the other hand\"}\n",
    "    words = word_tokenize(text)\n",
    "    count = sum(1 for w in words if w.lower() in markers)\n",
    "    return {\n",
    "        \"discourse_marker_count\": count,\n",
    "        \"discourse_marker_ratio\": count / len(words) if words else 0,\n",
    "    }\n",
    "\n",
    "\n",
    "def statistical_features(text: str) -> dict:\n",
    "    # Perplexity and burstiness\n",
    "    def _ppl(sent: str):\n",
    "        if _ppl_model is None: return None\n",
    "        enc = _ppl_tokenizer(sent, return_tensors='pt')\n",
    "        enc = {k: v.to(device) for k, v in enc.items()}\n",
    "        loss = _ppl_model(**enc, labels=enc['input_ids']).loss\n",
    "        return float(torch.exp(loss))\n",
    "    sentences = sent_tokenize(text)\n",
    "    ppvals = [_ppl(s) for s in sentences if _ppl(s) is not None]\n",
    "    return {\n",
    "        \"perplexity\": _ppl(text) if _ppl_model else None,\n",
    "        \"burstiness\": statistics.pstdev(ppvals) if len(ppvals) > 1 else 0,\n",
    "    }\n",
    "\n",
    "\n",
    "def repetition_features(text: str) -> dict:\n",
    "    tokens = [w.lower() for w in word_tokenize(text)]\n",
    "    def _ngram(n):\n",
    "        ngrams = list(zip(*(tokens[i:] for i in range(n))))\n",
    "        return 1 - len(set(ngrams)) / len(ngrams) if ngrams else 0\n",
    "    freq = Counter(tokens)\n",
    "    hapax = sum(1 for w,c in freq.items() if c == 1)\n",
    "    return {\n",
    "        \"bigram_repetition_ratio\": _ngram(2),\n",
    "        \"trigram_repetition_ratio\": _ngram(3),\n",
    "        \"hapax_legomena_ratio\": hapax / len(tokens) if tokens else 0,\n",
    "    }\n",
    "\n",
    "\n",
    "def syntactic_features(text: str) -> dict:\n",
    "    return {\n",
    "        \"present_participle_count\": sum(1 for w in word_tokenize(text) if w.lower().endswith('ing')),\n",
    "        \"passive_voice_count\": len(re.findall(r\"\\b(was|were|is|are|been|being)\\s+\\w+ed\\b\", text, flags=re.IGNORECASE)),\n",
    "    }\n",
    "\n",
    "\n",
    "def cohesion_features(text: str) -> dict:\n",
    "    text_l = text.lower()\n",
    "    return {\n",
    "        \"conjunction_count\": sum(text_l.count(w) for w in [\" and \", \" or \", \" but \", \" however \", \" because \", \" therefore \"]),\n",
    "        \"pronoun_count\": sum(text_l.count(p) for p in [\" i \", \" you \", \" he \", \" she \", \" they \", \" we \", \" it \"]),\n",
    "        \"contraction_count\": sum(len(re.findall(p, text)) for p in [r\"\\b\\w+n't\\b\", r\"\\b\\w+'re\\b\", r\"\\b\\w+'ve\\b\", r\"\\b\\w+'ll\\b\", r\"\\b\\w+'d\\b\"]),\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def extract_features_single_text(text: str) -> dict:\n",
    "    features = {}\n",
    "    features.update(lexical_features(text))\n",
    "    features.update(nlp_features(text))\n",
    "    features.update(readability_features(text))\n",
    "    features.update(stylometric_features(text))\n",
    "    features.update(discourse_features(text))\n",
    "    features.update(statistical_features(text))\n",
    "    features.update(repetition_features(text))\n",
    "    features.update(syntactic_features(text))\n",
    "    features.update(cohesion_features(text))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_features(texts):\n",
    "    results = []\n",
    "    for text in tqdm(texts):\n",
    "        features = extract_features_single_text(text)\n",
    "        results.append(features)\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_csv_paths(folder_path, recursive=False):\n",
    "    if recursive:\n",
    "        # Walk through all subdirectories\n",
    "        file_paths = [os.path.join(root, file) \n",
    "                      for root, _, files in os.walk(folder_path) \n",
    "                      for file in files if file.endswith('.csv')]\n",
    "    else:\n",
    "        # Get files in the root folder only\n",
    "        file_paths = [os.path.join(folder_path, file) \n",
    "                      for file in os.listdir(folder_path) \n",
    "                      if file.endswith('.csv')]\n",
    "    \n",
    "    return file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_feature_stats(df, stats, data_path, save_path):\n",
    "    df_stat = df.agg(stats).reset_index()\n",
    "\n",
    "    data_name, model = data_path.split(\"/\")[-1].split(\"_\")\n",
    "    model = model.removesuffix(\".csv\")\n",
    "\n",
    "    df_stat[\"model\"] = model\n",
    "    df_stat[\"data\"] = data_name\n",
    "    df_stat.rename(columns={\"index\": \"stat\"}, inplace=True)\n",
    "    df_stat.to_csv(save_path, mode=\"a\", index=False, header=not pd.io.common.file_exists(save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentile(n):\n",
    "    def percentile_(x):\n",
    "        return x.quantile(n)\n",
    "    percentile_.__name__ = 'percentile_{:02.0f}'.format(n*100)\n",
    "    return percentile_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_HUMAN_PATH = \"../data/data_human\"\n",
    "DATA_AI_PATH = \"../data/data_ai\"\n",
    "FEATURES_PATH = \"../data/features/\"\n",
    "FEATURES_STATS_PATH = \"../data/features/features_stats_master.csv\"\n",
    "STATS = ['mean', 'std', 'min', 'max', 'median', 'skew', 'kurtosis', 'var', percentile(0.1), percentile(0.2), percentile(0.3), percentile(0.4), percentile(0.5), percentile(0.6), percentile(0.7), percentile(0.8), percentile(0.9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = get_csv_paths(DATA_HUMAN_PATH) + get_csv_paths(DATA_AI_PATH, recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 356/3000 [00:03<00:29, 89.85it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(path)\n\u001b[1;32m      8\u001b[0m texts \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues[:\u001b[38;5;241m3000\u001b[39m]\n\u001b[0;32m----> 9\u001b[0m df_features \u001b[38;5;241m=\u001b[39m \u001b[43mcalc_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m df_features\u001b[38;5;241m.\u001b[39mto_csv(features_path, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     12\u001b[0m save_feature_stats(df_features, STATS, path, FEATURES_STATS_PATH)\n",
      "Cell \u001b[0;32mIn[47], line 4\u001b[0m, in \u001b[0;36mcalc_features\u001b[0;34m(texts)\u001b[0m\n\u001b[1;32m      2\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m tqdm(texts):\n\u001b[0;32m----> 4\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[43mextract_features_single_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(features)\n\u001b[1;32m      7\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(results)\n",
      "Cell \u001b[0;32mIn[46], line 76\u001b[0m, in \u001b[0;36mextract_features_single_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m#features.update(syntactic_features(text))\u001b[39;00m\n\u001b[1;32m     75\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate(readability_features(text))\n\u001b[0;32m---> 76\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate(\u001b[43mstylometric_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     77\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate(discourse_features(text))\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m features\n",
      "Cell \u001b[0;32mIn[46], line 48\u001b[0m, in \u001b[0;36mstylometric_features\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstylometric_features\u001b[39m(text):\n\u001b[0;32m---> 48\u001b[0m     words \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     bigrams \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(nltk\u001b[38;5;241m.\u001b[39mbigrams(words))\n\u001b[1;32m     50\u001b[0m     trigrams \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(nltk\u001b[38;5;241m.\u001b[39mtrigrams(words))\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/nltk/tokenize/__init__.py:144\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03mReturn a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03musing NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m:type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    142\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m--> 144\u001b[0m     token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_treebank_word_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m ]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/nltk/tokenize/destructive.py:179\u001b[0m, in \u001b[0;36mNLTKWordTokenizer.tokenize\u001b[0;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[1;32m    176\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m text \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m regexp, substitution \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mENDING_QUOTES:\n\u001b[0;32m--> 179\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mregexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubstitution\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m regexp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCONTRACTIONS2:\n\u001b[1;32m    182\u001b[0m     text \u001b[38;5;241m=\u001b[39m regexp\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m1 \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m2 \u001b[39m\u001b[38;5;124m\"\u001b[39m, text)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for path in paths:\n",
    "    if path.split(\"_\")[-1] == \"human.csv\":\n",
    "        features_path = os.path.join(FEATURES_PATH, path.split(\"/\")[-2], path.split(\"/\")[-1].replace(\".csv\", \"_features.csv\"))\n",
    "    else:\n",
    "        features_path = os.path.join(FEATURES_PATH, path.split(\"/\")[-3], path.split(\"/\")[-2], path.split(\"/\")[-1].replace(\".csv\", \"_features.csv\"))\n",
    "\n",
    "    df = pd.read_csv(path)\n",
    "    texts = df[\"text\"].values[:3000]\n",
    "    df_features = calc_features(texts)\n",
    "    df_features.to_csv(features_path, index=False)\n",
    "\n",
    "    save_feature_stats(df_features, STATS, path, FEATURES_STATS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
