{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import nltk\n",
    "import spacy\n",
    "import textstat\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Download necessary NLTK data\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load SpaCy model for syntactic features\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lexical Features\n",
    "def lexical_features(text):\n",
    "    words = word_tokenize(text)\n",
    "    sentences = sent_tokenize(text)\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    unique_words = set(words)\n",
    "    \n",
    "    features = {\n",
    "        \"word_count\": len(words),\n",
    "        \"character_count\": sum(len(word) for word in words),\n",
    "        \"average_word_length\": sum(len(word) for word in words) / len(words),\n",
    "        \"sentence_count\": len(sentences),\n",
    "        \"unique_words_ratio\": len(unique_words) / len(words),\n",
    "        \"stopword_ratio\": len([word for word in words if word.lower() in stop_words]) / len(words)\n",
    "    }\n",
    "    return features\n",
    "\n",
    "# Syntactic Features\n",
    "def syntactic_features(text):\n",
    "    doc = nlp(text)\n",
    "    pos_counts = Counter([token.pos_ for token in doc])\n",
    "    \n",
    "    features = {\n",
    "        \"noun_ratio\": pos_counts.get(\"NOUN\", 0) / len(doc),\n",
    "        \"verb_ratio\": pos_counts.get(\"VERB\", 0) / len(doc),\n",
    "        \"adjective_ratio\": pos_counts.get(\"ADJ\", 0) / len(doc),\n",
    "        \"average_sentence_length\": sum(len(sent.text.split()) for sent in doc.sents) / len(list(doc.sents)),\n",
    "        \"entity_count\": len(doc.ents)\n",
    "    }\n",
    "    return features\n",
    "\n",
    "# Semantic Features\n",
    "def semantic_features(text):\n",
    "    doc = nlp(text)\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    \n",
    "    features = {\n",
    "        \"named_entities\": entities,\n",
    "        \"entity_count\": len(entities)\n",
    "    }\n",
    "    return features\n",
    "\n",
    "# Readability Scores\n",
    "def readability_features(text):\n",
    "    features = {\n",
    "        \"flesch_reading_ease\": textstat.flesch_reading_ease(text),\n",
    "        \"gunning_fog_index\": textstat.gunning_fog(text),\n",
    "        \"smog_index\": textstat.smog_index(text),\n",
    "        \"automated_readability_index\": textstat.automated_readability_index(text),\n",
    "        \"dale_chall_readbility\": textstat.dale_chall_readability_score(text),\n",
    "\n",
    "    }\n",
    "    return features\n",
    "\n",
    "# Stylometric Features\n",
    "def stylometric_features(text):\n",
    "    words = word_tokenize(text)\n",
    "    bigrams = list(nltk.bigrams(words))\n",
    "    trigrams = list(nltk.trigrams(words))\n",
    "    \n",
    "    features = {\n",
    "        \"bigram_count\": len(bigrams),\n",
    "        \"trigrams_count\": len(trigrams),\n",
    "        \"punctuation_count\": sum(1 for char in text if char in \".,;!?\")\n",
    "    }\n",
    "    return features\n",
    "\n",
    "# Combine all features for one text\n",
    "def extract_features_single_text(text):\n",
    "    features = {}\n",
    "    features.update(lexical_features(text))\n",
    "    # features.update(syntactic_features(text)) # takes majority of compute time\n",
    "    # features.update(semantic_features(text))\n",
    "    features.update(readability_features(text))\n",
    "    features.update(stylometric_features(text))\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_features(texts):\n",
    "    results = []\n",
    "    for text in tqdm(texts):\n",
    "        features = extract_features_single_text(text)\n",
    "        results.append(features)\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_csv_paths(folder_path, recursive=False):\n",
    "    if recursive:\n",
    "        # Walk through all subdirectories\n",
    "        file_paths = [os.path.join(root, file) \n",
    "                      for root, _, files in os.walk(folder_path) \n",
    "                      for file in files if file.endswith('.csv')]\n",
    "    else:\n",
    "        # Get files in the root folder only\n",
    "        file_paths = [os.path.join(folder_path, file) \n",
    "                      for file in os.listdir(folder_path) \n",
    "                      if file.endswith('.csv')]\n",
    "    \n",
    "    return file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_feature_stats(df, stats, data_path, save_path):\n",
    "    df_stat = df.agg(stats).reset_index()\n",
    "\n",
    "    data_name, model = data_path.split(\"/\")[-1].split(\"_\")\n",
    "    model = model.removesuffix(\".csv\")\n",
    "\n",
    "    df_stat[\"model\"] = model\n",
    "    df_stat[\"data\"] = data_name\n",
    "    df_stat.rename(columns={\"index\": \"stat\"}, inplace=True)\n",
    "    df_stat.to_csv(save_path, mode=\"a\", index=False, header=not pd.io.common.file_exists(save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentile(n):\n",
    "    def percentile_(x):\n",
    "        return x.quantile(n)\n",
    "    percentile_.__name__ = 'percentile_{:02.0f}'.format(n*100)\n",
    "    return percentile_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_HUMAN_PATH = \"../data/data_human\"\n",
    "DATA_AI_PATH = \"../data/data_ai\"\n",
    "FEATURES_PATH = \"../data/features/\"\n",
    "STATS_PATH = \"../data/features/features_stats_master.csv\"\n",
    "STATS = ['mean', 'std', 'min', 'max', 'median', 'skew', 'kurtosis', 'var', percentile(0.1), percentile(0.2), percentile(0.3), percentile(0.4), percentile(0.5), percentile(0.6), percentile(0.7), percentile(0.8), percentile(0.9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = get_csv_paths(DATA_HUMAN_PATH) + get_csv_paths(DATA_AI_PATH, recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [00:26<00:00, 111.72it/s]\n",
      "100%|██████████| 3000/3000 [00:38<00:00, 77.69it/s] \n",
      "100%|██████████| 3000/3000 [00:14<00:00, 205.80it/s]\n",
      "100%|██████████| 3000/3000 [00:01<00:00, 1932.83it/s]\n",
      "100%|██████████| 3000/3000 [00:03<00:00, 913.13it/s] \n",
      "100%|██████████| 3000/3000 [00:05<00:00, 523.53it/s]\n",
      "100%|██████████| 3000/3000 [00:15<00:00, 192.25it/s]\n",
      "100%|██████████| 3000/3000 [00:02<00:00, 1381.62it/s]\n",
      "100%|██████████| 2638/2638 [00:35<00:00, 74.48it/s] \n",
      "100%|██████████| 384/384 [00:02<00:00, 148.52it/s]\n",
      "100%|██████████| 384/384 [00:01<00:00, 340.23it/s]\n",
      "100%|██████████| 24/24 [00:00<00:00, 103.95it/s]\n",
      "100%|██████████| 24/24 [00:00<00:00, 1130.51it/s]\n",
      "100%|██████████| 24/24 [00:00<00:00, 444.72it/s]\n",
      "100%|██████████| 24/24 [00:00<00:00, 815.84it/s]\n",
      "100%|██████████| 24/24 [00:00<00:00, 434.17it/s]\n",
      "100%|██████████| 24/24 [00:00<00:00, 177.39it/s]\n",
      "100%|██████████| 24/24 [00:00<00:00, 139.46it/s]\n"
     ]
    }
   ],
   "source": [
    "for path in paths:\n",
    "    if path.split(\"_\")[-1] == \"human.csv\":\n",
    "        features_path = os.path.join(FEATURES_PATH, path.split(\"/\")[-2], path.split(\"/\")[-1].replace(\".csv\", \"_features.csv\"))\n",
    "    else:\n",
    "        features_path = os.path.join(FEATURES_PATH, path.split(\"/\")[-3], path.split(\"/\")[-2], path.split(\"/\")[-1].replace(\".csv\", \"_features.csv\"))\n",
    "\n",
    "    df = pd.read_csv(path)\n",
    "    texts = df[\"text\"].values[:3000]\n",
    "    df_features = calc_features(texts)\n",
    "    df_features.to_csv(features_path, index=False)\n",
    "\n",
    "    save_feature_stats(df_features, STATS, path, STATS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
