{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "\n",
    "\n",
    "class FineTuneClassifier(nn.Module):\n",
    "    def __init__(self, base_model_path: str, num_labels: int) -> None:\n",
    "        super(FineTuneClassifier, self).__init__()\n",
    "        self.base_model = AutoModel.from_pretrained(base_model_path)\n",
    "\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.classifier = nn.Linear(self.base_model.config.hidden_size * 2, num_labels)\n",
    "\n",
    "    @classmethod\n",
    "    def from_classifier_head(\n",
    "        cls, base_model_path: str, path: str, num_labels: int\n",
    "    ) -> nn.Module:\n",
    "        model = cls(base_model_path, num_labels)\n",
    "        model.classifier.load_state_dict(torch.load(path))\n",
    "        return model\n",
    "\n",
    "    def forward(\n",
    "        self, input_ids: torch.tensor, attention_mask: torch.tensor\n",
    "    ) -> torch.tensor:\n",
    "        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        B, T, C = outputs.logits.shape\n",
    "\n",
    "        all_tokens_hidden = outputs.logits  # (B, T, C)\n",
    "        last_token_hidden = outputs.logits[:, -1, :]  # (B, C)\n",
    "        last_token_hidden = last_token_hidden.unsqueeze(1).expand(B, T, C)\n",
    "\n",
    "        combined_representation = torch.cat(\n",
    "            (all_tokens_hidden, last_token_hidden), dim=-1\n",
    "        )\n",
    "        logits = self.classifier(combined_representation)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class BaselineClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        num_layers: int,\n",
    "        nhead: int,\n",
    "        max_seq_length: int,\n",
    "        vocab_size: int,\n",
    "        pad_token_id: int,\n",
    "        num_labels: int,\n",
    "    ) -> None:\n",
    "        super(BaselineClassifier, self).__init__()\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.token_embedding = nn.Embedding(\n",
    "            vocab_size, d_model, padding_idx=pad_token_id\n",
    "        )\n",
    "        self.pos_embedding = nn.Embedding(max_seq_length, d_model)\n",
    "        decoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(decoder_layer, num_layers=num_layers)\n",
    "        self.classifier = nn.Linear(d_model * 2, num_labels)\n",
    "\n",
    "    def forward(self, token_ids: torch.tensor) -> torch.tensor:\n",
    "        batch_size, seq_len = token_ids.shape\n",
    "\n",
    "        token_emb = self.token_embedding(token_ids)\n",
    "        pos_ids = torch.arange(seq_len, device=token_ids.device).unsqueeze(0)\n",
    "        pos_emb = self.pos_embedding(pos_ids)\n",
    "        embeddings = token_emb + pos_emb\n",
    "\n",
    "        causal_mask = torch.triu(\n",
    "            torch.ones(seq_len, seq_len, device=token_ids.device, dtype=torch.bool),\n",
    "            diagonal=1,\n",
    "        )\n",
    "\n",
    "        pad_mask = token_ids.eq(self.pad_token_id)  # shape: (batch_size, seq_len)\n",
    "\n",
    "        output = self.transformer(\n",
    "            embeddings, mask=causal_mask, src_key_padding_mask=pad_mask\n",
    "        )\n",
    "\n",
    "        B, T, C = output.shape\n",
    "        all_tokens_hidden = output  # (B, T, C)\n",
    "        last_token_hidden = output[:, -1, :]  # (B, C)\n",
    "        last_token_hidden = last_token_hidden.unsqueeze(1).expand(B, T, C)\n",
    "\n",
    "        combined_representation = torch.cat(\n",
    "            (all_tokens_hidden, last_token_hidden), dim=-1\n",
    "        )\n",
    "        logits = self.classifier(combined_representation)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "BASELINE_MODELS: Dict[str, Dict[str, int]] = {\n",
    "    \"mini\": {\n",
    "        \"d_model\": 64,\n",
    "        \"num_layers\": 4,\n",
    "        \"num_heads\": 4,\n",
    "        \"max_len\": 16_384,\n",
    "    },\n",
    "    \"small\": {\n",
    "        \"d_model\": 510,\n",
    "        \"num_layers\": 8,\n",
    "        \"num_heads\": 6,\n",
    "        \"max_len\": 16_384,\n",
    "    },\n",
    "    \"medium\": {\n",
    "        \"d_model\": 1344,\n",
    "        \"num_layers\": 24,\n",
    "        \"num_heads\": 16,\n",
    "        \"max_len\": 16_384,\n",
    "    },\n",
    "    \"large\": {\n",
    "        \"d_model\": 1824,\n",
    "        \"num_layers\": 36,\n",
    "        \"num_heads\": 24,\n",
    "        \"max_len\": 16_384,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: mini, Total Parameters: 10.49M, % emb parameters: 0.7928761601769944\n",
      "Active params: 2.17M, emb params: 8.32M\n",
      "Used VRAM: 40.30 MB\n",
      "Model: small, Total Parameters: 99.75M, % emb parameters: 0.6646892012547166\n",
      "Active params: 33.45M, emb params: 66.30M\n",
      "Used VRAM: 380.63 MB\n",
      "Model: medium, Total Parameters: 502.61M, % emb parameters: 0.34762279350401076\n",
      "Active params: 327.89M, emb params: 174.72M\n",
      "Used VRAM: 1918.55 MB\n",
      "Model: large, Total Parameters: 1015.72M, % emb parameters: 0.23345000116597012\n",
      "Active params: 778.60M, emb params: 237.12M\n",
      "Used VRAM: 3874.75 MB\n"
     ]
    }
   ],
   "source": [
    "tmp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tmp():\n",
    "    for name, config in BASELINE_MODELS.items():\n",
    "        d_model = config[\"d_model\"]\n",
    "        num_layers = config[\"num_layers\"]\n",
    "        nhead = config[\"num_heads\"]\n",
    "        max_seq_length = config[\"max_len\"]\n",
    "        vocab_size = 130_000\n",
    "        pad_token_id = 0\n",
    "        num_labels = 2\n",
    "\n",
    "        model = BaselineClassifier(\n",
    "            d_model,\n",
    "            num_layers,\n",
    "            nhead,\n",
    "            max_seq_length,\n",
    "            vocab_size,\n",
    "            pad_token_id,\n",
    "            num_labels,\n",
    "        )\n",
    "        total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        emb_params = vocab_size * d_model\n",
    "\n",
    "        model.cuda()\n",
    "        used_memory = torch.cuda.memory_allocated(device=torch.device(\"cuda:0\"))\n",
    "        print(f\"Model: {name}, Total Parameters: {total_params / 1e6:.2f}M, % emb parameters: {emb_params/total_params}\")\n",
    "        print(f\"Active params: {(total_params - emb_params) / 1e6:.2f}M, emb params: {emb_params / 1e6:.2f}M\")\n",
    "        print(f\"Used VRAM: {used_memory / (1024 ** 2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>model</th>\n",
       "      <th>num_samples</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_chars</th>\n",
       "      <th>num_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nyt-comments</td>\n",
       "      <td>human</td>\n",
       "      <td>4223213</td>\n",
       "      <td>18713269</td>\n",
       "      <td>342590681</td>\n",
       "      <td>1418281599</td>\n",
       "      <td>367129360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blogs</td>\n",
       "      <td>human</td>\n",
       "      <td>576731</td>\n",
       "      <td>8328325</td>\n",
       "      <td>150710195</td>\n",
       "      <td>557327652</td>\n",
       "      <td>164361476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>raid</td>\n",
       "      <td>human</td>\n",
       "      <td>138244</td>\n",
       "      <td>1808791</td>\n",
       "      <td>46559309</td>\n",
       "      <td>215280947</td>\n",
       "      <td>95664352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>natural-questions</td>\n",
       "      <td>human</td>\n",
       "      <td>231628</td>\n",
       "      <td>544545</td>\n",
       "      <td>12325780</td>\n",
       "      <td>52665339</td>\n",
       "      <td>14758108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>writingprompts</td>\n",
       "      <td>human</td>\n",
       "      <td>303140</td>\n",
       "      <td>13802625</td>\n",
       "      <td>196423260</td>\n",
       "      <td>721935184</td>\n",
       "      <td>209317891</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                data  model  num_samples  num_sentences  num_words  \\\n",
       "0       nyt-comments  human      4223213       18713269  342590681   \n",
       "1              blogs  human       576731        8328325  150710195   \n",
       "2               raid  human       138244        1808791   46559309   \n",
       "3  natural-questions  human       231628         544545   12325780   \n",
       "4     writingprompts  human       303140       13802625  196423260   \n",
       "\n",
       "    num_chars  num_tokens  \n",
       "0  1418281599   367129360  \n",
       "1   557327652   164361476  \n",
       "2   215280947    95664352  \n",
       "3    52665339    14758108  \n",
       "4   721935184   209317891  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/stats/data_stats_master.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>num_samples</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blogs</td>\n",
       "      <td>576731</td>\n",
       "      <td>8328325</td>\n",
       "      <td>150710195</td>\n",
       "      <td>164361476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>essays</td>\n",
       "      <td>2638</td>\n",
       "      <td>123010</td>\n",
       "      <td>1883626</td>\n",
       "      <td>1910971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>natural-questions</td>\n",
       "      <td>231628</td>\n",
       "      <td>544545</td>\n",
       "      <td>12325780</td>\n",
       "      <td>14758108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nyt-articles</td>\n",
       "      <td>15813</td>\n",
       "      <td>21318</td>\n",
       "      <td>397111</td>\n",
       "      <td>421258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nyt-comments</td>\n",
       "      <td>4223213</td>\n",
       "      <td>18713269</td>\n",
       "      <td>342590681</td>\n",
       "      <td>367129360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>raid</td>\n",
       "      <td>138244</td>\n",
       "      <td>1808791</td>\n",
       "      <td>46559309</td>\n",
       "      <td>95664352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>reddit</td>\n",
       "      <td>655484</td>\n",
       "      <td>1817601</td>\n",
       "      <td>30286813</td>\n",
       "      <td>32563547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tweets</td>\n",
       "      <td>389916</td>\n",
       "      <td>735759</td>\n",
       "      <td>7479175</td>\n",
       "      <td>8375173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>writingprompts</td>\n",
       "      <td>303140</td>\n",
       "      <td>13802625</td>\n",
       "      <td>196423260</td>\n",
       "      <td>209317891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>xsum</td>\n",
       "      <td>226394</td>\n",
       "      <td>4298208</td>\n",
       "      <td>97475080</td>\n",
       "      <td>105943034</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                data  num_samples  num_sentences  num_words  num_tokens\n",
       "1              blogs       576731        8328325  150710195   164361476\n",
       "5             essays         2638         123010    1883626     1910971\n",
       "3  natural-questions       231628         544545   12325780    14758108\n",
       "6       nyt-articles        15813          21318     397111      421258\n",
       "0       nyt-comments      4223213       18713269  342590681   367129360\n",
       "2               raid       138244        1808791   46559309    95664352\n",
       "8             reddit       655484        1817601   30286813    32563547\n",
       "7             tweets       389916         735759    7479175     8375173\n",
       "4     writingprompts       303140       13802625  196423260   209317891\n",
       "9               xsum       226394        4298208   97475080   105943034"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"model\"] == \"human\"][[\"data\", \"num_samples\", \"num_sentences\", \"num_words\", \"num_tokens\"]].sort_values(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data             nyt-commentsblogsraidnatural-questionswritingp...\n",
       "num_samples                                                6763201\n",
       "num_sentences                                             50193451\n",
       "num_words                                                886131030\n",
       "num_tokens                                              1000445170\n",
       "dtype: object"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"model\"] == \"human\"][[\"data\", \"num_samples\", \"num_sentences\", \"num_words\", \"num_tokens\"]].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>num_samples</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blogs</td>\n",
       "      <td>1182270</td>\n",
       "      <td>21203468</td>\n",
       "      <td>381085553</td>\n",
       "      <td>407027203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>essays</td>\n",
       "      <td>58036</td>\n",
       "      <td>2765379</td>\n",
       "      <td>40240176</td>\n",
       "      <td>39439570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>natural-questions</td>\n",
       "      <td>474227</td>\n",
       "      <td>2279452</td>\n",
       "      <td>46437493</td>\n",
       "      <td>58791916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nyt-articles</td>\n",
       "      <td>347885</td>\n",
       "      <td>2165238</td>\n",
       "      <td>60102771</td>\n",
       "      <td>63994699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nyt-comments</td>\n",
       "      <td>8657565</td>\n",
       "      <td>36261238</td>\n",
       "      <td>705529033</td>\n",
       "      <td>746827854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>raid</td>\n",
       "      <td>864020</td>\n",
       "      <td>10784314</td>\n",
       "      <td>264179188</td>\n",
       "      <td>332672877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>reddit</td>\n",
       "      <td>3408276</td>\n",
       "      <td>14035572</td>\n",
       "      <td>296477342</td>\n",
       "      <td>340254517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tweets</td>\n",
       "      <td>3665193</td>\n",
       "      <td>8117363</td>\n",
       "      <td>97242239</td>\n",
       "      <td>117976085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>writingprompts</td>\n",
       "      <td>621437</td>\n",
       "      <td>23319770</td>\n",
       "      <td>379307258</td>\n",
       "      <td>399447670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>xsum</td>\n",
       "      <td>939528</td>\n",
       "      <td>12847512</td>\n",
       "      <td>339556027</td>\n",
       "      <td>360436581</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                data  num_samples  num_sentences  num_words  num_tokens\n",
       "0              blogs      1182270       21203468  381085553   407027203\n",
       "1             essays        58036        2765379   40240176    39439570\n",
       "2  natural-questions       474227        2279452   46437493    58791916\n",
       "3       nyt-articles       347885        2165238   60102771    63994699\n",
       "4       nyt-comments      8657565       36261238  705529033   746827854\n",
       "5               raid       864020       10784314  264179188   332672877\n",
       "6             reddit      3408276       14035572  296477342   340254517\n",
       "7             tweets      3665193        8117363   97242239   117976085\n",
       "8     writingprompts       621437       23319770  379307258   399447670\n",
       "9               xsum       939528       12847512  339556027   360436581"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"data\").sum().reset_index()[[\"data\", \"num_samples\", \"num_sentences\", \"num_words\", \"num_tokens\"]].sort_values(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data             blogsessaysnatural-questionsnyt-articlesnyt-co...\n",
       "num_samples                                               20218437\n",
       "num_sentences                                            133779306\n",
       "num_words                                               2610157080\n",
       "num_tokens                                              2866868972\n",
       "dtype: object"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"data\").sum().reset_index()[[\"data\", \"num_samples\", \"num_sentences\", \"num_words\", \"num_tokens\"]].sort_values(\"data\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>num_samples</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Falcon3-3B-Instruct</td>\n",
       "      <td>640766</td>\n",
       "      <td>3161253</td>\n",
       "      <td>67838176</td>\n",
       "      <td>72520075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Falcon3-7B-Instruct</td>\n",
       "      <td>640767</td>\n",
       "      <td>3176223</td>\n",
       "      <td>67428463</td>\n",
       "      <td>71932999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Llama-3.1-8B-Instruct</td>\n",
       "      <td>640541</td>\n",
       "      <td>3050731</td>\n",
       "      <td>70507468</td>\n",
       "      <td>73979179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>640759</td>\n",
       "      <td>3045728</td>\n",
       "      <td>71913423</td>\n",
       "      <td>75775399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Meta-Llama-3.1-70B-Instruct-AWQ-INT4</td>\n",
       "      <td>640750</td>\n",
       "      <td>2994366</td>\n",
       "      <td>64801390</td>\n",
       "      <td>67950245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Meta-Llama-3.3-70B-Instruct-AWQ-INT4</td>\n",
       "      <td>640766</td>\n",
       "      <td>2861116</td>\n",
       "      <td>69234095</td>\n",
       "      <td>72319091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ministral-8B-Instruct-2410</td>\n",
       "      <td>640767</td>\n",
       "      <td>5700407</td>\n",
       "      <td>94563415</td>\n",
       "      <td>98615451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Mistral-Nemo-Instruct-2407</td>\n",
       "      <td>640765</td>\n",
       "      <td>4684474</td>\n",
       "      <td>86356229</td>\n",
       "      <td>91252590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Phi-3-medium-128k-instruct</td>\n",
       "      <td>640193</td>\n",
       "      <td>4937858</td>\n",
       "      <td>100324679</td>\n",
       "      <td>107425744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Phi-3-mini-128k-instruct</td>\n",
       "      <td>640760</td>\n",
       "      <td>5390937</td>\n",
       "      <td>87882920</td>\n",
       "      <td>92987021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Phi-3-small-128k-instruct</td>\n",
       "      <td>640761</td>\n",
       "      <td>4930235</td>\n",
       "      <td>103899993</td>\n",
       "      <td>121526770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Phi-3.5-mini-instruct</td>\n",
       "      <td>640749</td>\n",
       "      <td>7208419</td>\n",
       "      <td>125067256</td>\n",
       "      <td>136315230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Phi-4-mini-instruct</td>\n",
       "      <td>640767</td>\n",
       "      <td>4605149</td>\n",
       "      <td>81379425</td>\n",
       "      <td>88354254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Qwen2-72B-Instruct-AWQ</td>\n",
       "      <td>640766</td>\n",
       "      <td>3964097</td>\n",
       "      <td>81523750</td>\n",
       "      <td>86351634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Qwen2-7B-Instruct</td>\n",
       "      <td>640766</td>\n",
       "      <td>3666534</td>\n",
       "      <td>81721207</td>\n",
       "      <td>85679403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Qwen2.5-14B-Instruct</td>\n",
       "      <td>640767</td>\n",
       "      <td>2995544</td>\n",
       "      <td>59549683</td>\n",
       "      <td>63018459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Qwen2.5-3B-Instruct</td>\n",
       "      <td>640766</td>\n",
       "      <td>2998835</td>\n",
       "      <td>57828102</td>\n",
       "      <td>61808590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Qwen2.5-72B-Instruct-AWQ</td>\n",
       "      <td>640767</td>\n",
       "      <td>3138922</td>\n",
       "      <td>62068965</td>\n",
       "      <td>64882569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Qwen2.5-7B-Instruct</td>\n",
       "      <td>640767</td>\n",
       "      <td>3190146</td>\n",
       "      <td>60745114</td>\n",
       "      <td>64603482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>gpt-4.1-nano-2025-04-14</td>\n",
       "      <td>640767</td>\n",
       "      <td>2426004</td>\n",
       "      <td>45100444</td>\n",
       "      <td>47038459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>human</td>\n",
       "      <td>6763201</td>\n",
       "      <td>50193451</td>\n",
       "      <td>886131030</td>\n",
       "      <td>1000445170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>phi-4</td>\n",
       "      <td>640759</td>\n",
       "      <td>5458877</td>\n",
       "      <td>184291853</td>\n",
       "      <td>222087158</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   model  num_samples  num_sentences  \\\n",
       "0                    Falcon3-3B-Instruct       640766        3161253   \n",
       "1                    Falcon3-7B-Instruct       640767        3176223   \n",
       "2                  Llama-3.1-8B-Instruct       640541        3050731   \n",
       "3                  Llama-3.2-3B-Instruct       640759        3045728   \n",
       "4   Meta-Llama-3.1-70B-Instruct-AWQ-INT4       640750        2994366   \n",
       "5   Meta-Llama-3.3-70B-Instruct-AWQ-INT4       640766        2861116   \n",
       "6             Ministral-8B-Instruct-2410       640767        5700407   \n",
       "7             Mistral-Nemo-Instruct-2407       640765        4684474   \n",
       "8             Phi-3-medium-128k-instruct       640193        4937858   \n",
       "9               Phi-3-mini-128k-instruct       640760        5390937   \n",
       "10             Phi-3-small-128k-instruct       640761        4930235   \n",
       "11                 Phi-3.5-mini-instruct       640749        7208419   \n",
       "12                   Phi-4-mini-instruct       640767        4605149   \n",
       "13                Qwen2-72B-Instruct-AWQ       640766        3964097   \n",
       "14                     Qwen2-7B-Instruct       640766        3666534   \n",
       "15                  Qwen2.5-14B-Instruct       640767        2995544   \n",
       "16                   Qwen2.5-3B-Instruct       640766        2998835   \n",
       "17              Qwen2.5-72B-Instruct-AWQ       640767        3138922   \n",
       "18                   Qwen2.5-7B-Instruct       640767        3190146   \n",
       "19               gpt-4.1-nano-2025-04-14       640767        2426004   \n",
       "20                                 human      6763201       50193451   \n",
       "21                                 phi-4       640759        5458877   \n",
       "\n",
       "    num_words  num_tokens  \n",
       "0    67838176    72520075  \n",
       "1    67428463    71932999  \n",
       "2    70507468    73979179  \n",
       "3    71913423    75775399  \n",
       "4    64801390    67950245  \n",
       "5    69234095    72319091  \n",
       "6    94563415    98615451  \n",
       "7    86356229    91252590  \n",
       "8   100324679   107425744  \n",
       "9    87882920    92987021  \n",
       "10  103899993   121526770  \n",
       "11  125067256   136315230  \n",
       "12   81379425    88354254  \n",
       "13   81523750    86351634  \n",
       "14   81721207    85679403  \n",
       "15   59549683    63018459  \n",
       "16   57828102    61808590  \n",
       "17   62068965    64882569  \n",
       "18   60745114    64603482  \n",
       "19   45100444    47038459  \n",
       "20  886131030  1000445170  \n",
       "21  184291853   222087158  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"model\").sum().reset_index()[[\"model\", \"num_samples\", \"num_sentences\", \"num_words\", \"num_tokens\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model            Falcon3-3B-InstructFalcon3-7B-InstructLlama-3....\n",
       "num_samples                                               20218437\n",
       "num_sentences                                            133779306\n",
       "num_words                                               2610157080\n",
       "num_tokens                                              2866868972\n",
       "dtype: object"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"model\").sum().reset_index()[[\"model\", \"num_samples\", \"num_sentences\", \"num_words\", \"num_tokens\"]].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"is_human\"] = df[\"model\"].apply(lambda x: 1 if x == \"human\" else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_human</th>\n",
       "      <th>num_samples</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>13455236</td>\n",
       "      <td>83585855</td>\n",
       "      <td>1724026050</td>\n",
       "      <td>1866423802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>6763201</td>\n",
       "      <td>50193451</td>\n",
       "      <td>886131030</td>\n",
       "      <td>1000445170</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_human  num_samples  num_sentences   num_words  num_tokens\n",
       "0         0     13455236       83585855  1724026050  1866423802\n",
       "1         1      6763201       50193451   886131030  1000445170"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"is_human\").sum()[[\"num_samples\", \"num_sentences\", \"num_words\", \"num_tokens\"]].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "\n",
    "\n",
    "class FineTuneClassifier(nn.Module):\n",
    "    def __init__(self, base_model_path: str, num_labels: int) -> None:\n",
    "        super(FineTuneClassifier, self).__init__()\n",
    "        self.base_model = AutoModel.from_pretrained(base_model_path)\n",
    "\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.classifier = nn.Linear(self.base_model.config.hidden_size * 2, num_labels)\n",
    "\n",
    "    @classmethod\n",
    "    def from_classifier_head(\n",
    "        cls, base_model_path: str, path: str, num_labels: int\n",
    "    ) -> nn.Module:\n",
    "        model = cls(base_model_path, num_labels)\n",
    "        model.classifier.load_state_dict(torch.load(path))\n",
    "        return model\n",
    "\n",
    "    def forward(\n",
    "        self, input_ids: torch.tensor, attention_mask: torch.tensor\n",
    "    ) -> torch.tensor:\n",
    "        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        B, T, C = outputs.logits.shape\n",
    "\n",
    "        all_tokens_hidden = outputs.logits  # (B, T, C)\n",
    "        last_token_hidden = outputs.logits[:, -1, :]  # (B, C)\n",
    "        last_token_hidden = last_token_hidden.unsqueeze(1).expand(B, T, C)\n",
    "\n",
    "        combined_representation = torch.cat(\n",
    "            (all_tokens_hidden, last_token_hidden), dim=-1\n",
    "        )\n",
    "        logits = self.classifier(combined_representation)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class BaselineClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        num_layers: int,\n",
    "        nhead: int,\n",
    "        max_seq_length: int,\n",
    "        vocab_size: int,\n",
    "        pad_token_id: int,\n",
    "        num_labels: int,\n",
    "    ) -> None:\n",
    "        super(BaselineClassifier, self).__init__()\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.token_embedding = nn.Embedding(\n",
    "            vocab_size, d_model, padding_idx=pad_token_id\n",
    "        )\n",
    "        self.pos_embedding = nn.Embedding(max_seq_length, d_model)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=d_model, nhead=nhead, batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        self.classifier = nn.Linear(d_model * 2, num_labels)\n",
    "\n",
    "    def forward(self, token_ids: torch.tensor) -> torch.tensor:\n",
    "\n",
    "        token_emb = self.token_embedding(token_ids)\n",
    "\n",
    "        output = self.transformer(\n",
    "            token_emb, \n",
    "        )\n",
    "\n",
    "        B, T, C = output.shape\n",
    "        all_tokens_hidden = output  # (B, T, C)\n",
    "        last_token_hidden = output[:, -1, :]  # (B, C)\n",
    "        last_token_hidden = last_token_hidden.unsqueeze(1).expand(B, T, C)\n",
    "\n",
    "        combined_representation = torch.cat(\n",
    "            (all_tokens_hidden, last_token_hidden), dim=-1\n",
    "        )\n",
    "        logits = self.classifier(combined_representation)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'baseline_classifier.png'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torchviz import make_dot\n",
    "\n",
    "# Assuming your BaselineClassifier is defined\n",
    "model = BaselineClassifier(\n",
    "    d_model=128,\n",
    "    num_layers=2,\n",
    "    nhead=4,\n",
    "    max_seq_length=100,\n",
    "    vocab_size=30522,\n",
    "    pad_token_id=0,\n",
    "    num_labels=10\n",
    ")\n",
    "\n",
    "dummy_input = torch.randint(0, 30522, (2, 100))  # (batch_size, seq_len)\n",
    "output = model(dummy_input)\n",
    "\n",
    "# Create the computation graph\n",
    "dot = make_dot(output, params=dict(model.named_parameters()))\n",
    "dot.render(\"baseline_classifier\", format=\"png\")  # Saves a .png file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchview import draw_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: model Pages: 1 -->\n",
       "<svg width=\"283pt\" height=\"654pt\"\n",
       " viewBox=\"0.00 0.00 283.00 654.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 650)\">\n",
       "<title>model</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-650 279,-650 279,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<polygon fill=\"lightyellow\" stroke=\"transparent\" points=\"174.5,-646 56.5,-646 56.5,-614 174.5,-614 174.5,-646\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"56.5,-614 56.5,-646 126.5,-646 126.5,-614 56.5,-614\"/>\n",
       "<text text-anchor=\"start\" x=\"61.5\" y=\"-633\" font-family=\"Linux libertine\" font-size=\"10.00\">input&#45;tensor</text>\n",
       "<text text-anchor=\"start\" x=\"73\" y=\"-622\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:0</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"126.5,-614 126.5,-646 174.5,-646 174.5,-614 126.5,-614\"/>\n",
       "<text text-anchor=\"start\" x=\"131.5\" y=\"-627.5\" font-family=\"Linux libertine\" font-size=\"10.00\">(2, 100)</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"209.5,-578 21.5,-578 21.5,-536 209.5,-536 209.5,-578\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"21.5,-536 21.5,-578 86.5,-578 86.5,-536 21.5,-536\"/>\n",
       "<text text-anchor=\"start\" x=\"26.5\" y=\"-560\" font-family=\"Linux libertine\" font-size=\"10.00\">Embedding</text>\n",
       "<text text-anchor=\"start\" x=\"35.5\" y=\"-549\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:1</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"86.5,-557 86.5,-578 134.5,-578 134.5,-557 86.5,-557\"/>\n",
       "<text text-anchor=\"start\" x=\"96.5\" y=\"-565\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"134.5,-557 134.5,-578 209.5,-578 209.5,-557 134.5,-557\"/>\n",
       "<text text-anchor=\"start\" x=\"151.5\" y=\"-565\" font-family=\"Linux libertine\" font-size=\"10.00\">(2, 100) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"86.5,-536 86.5,-557 134.5,-557 134.5,-536 86.5,-536\"/>\n",
       "<text text-anchor=\"start\" x=\"91.5\" y=\"-544\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"134.5,-536 134.5,-557 209.5,-557 209.5,-536 134.5,-536\"/>\n",
       "<text text-anchor=\"start\" x=\"139.5\" y=\"-544\" font-family=\"Linux libertine\" font-size=\"10.00\">(2, 100, 128) </text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M115.5,-613.94C115.5,-606.45 115.5,-597.12 115.5,-588.24\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"119,-588.16 115.5,-578.16 112,-588.16 119,-588.16\"/>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"231,-500 0,-500 0,-458 231,-458 231,-500\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"0.5,-458 0.5,-500 108.5,-500 108.5,-458 0.5,-458\"/>\n",
       "<text text-anchor=\"start\" x=\"5.5\" y=\"-482\" font-family=\"Linux libertine\" font-size=\"10.00\">TransformerEncoder</text>\n",
       "<text text-anchor=\"start\" x=\"36\" y=\"-471\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:1</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"108.5,-479 108.5,-500 156.5,-500 156.5,-479 108.5,-479\"/>\n",
       "<text text-anchor=\"start\" x=\"118.5\" y=\"-487\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"156.5,-479 156.5,-500 231.5,-500 231.5,-479 156.5,-479\"/>\n",
       "<text text-anchor=\"start\" x=\"161.5\" y=\"-487\" font-family=\"Linux libertine\" font-size=\"10.00\">(2, 100, 128) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"108.5,-458 108.5,-479 156.5,-479 156.5,-458 108.5,-458\"/>\n",
       "<text text-anchor=\"start\" x=\"113.5\" y=\"-466\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"156.5,-458 156.5,-479 231.5,-479 231.5,-458 156.5,-458\"/>\n",
       "<text text-anchor=\"start\" x=\"161.5\" y=\"-466\" font-family=\"Linux libertine\" font-size=\"10.00\">(2, 100, 128) </text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>1&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M115.5,-535.63C115.5,-527.82 115.5,-518.73 115.5,-510.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"119,-510.16 115.5,-500.16 112,-510.16 119,-510.16\"/>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>3</title>\n",
       "<polygon fill=\"aliceblue\" stroke=\"transparent\" points=\"275,-422 82,-422 82,-380 275,-380 275,-422\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"82.5,-380 82.5,-422 152.5,-422 152.5,-380 82.5,-380\"/>\n",
       "<text text-anchor=\"start\" x=\"87.5\" y=\"-404\" font-family=\"Linux libertine\" font-size=\"10.00\">__getitem__</text>\n",
       "<text text-anchor=\"start\" x=\"99\" y=\"-393\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:1</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"152.5,-401 152.5,-422 200.5,-422 200.5,-401 152.5,-401\"/>\n",
       "<text text-anchor=\"start\" x=\"162.5\" y=\"-409\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"200.5,-401 200.5,-422 275.5,-422 275.5,-401 200.5,-401\"/>\n",
       "<text text-anchor=\"start\" x=\"205.5\" y=\"-409\" font-family=\"Linux libertine\" font-size=\"10.00\">(2, 100, 128) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"152.5,-380 152.5,-401 200.5,-401 200.5,-380 152.5,-380\"/>\n",
       "<text text-anchor=\"start\" x=\"157.5\" y=\"-388\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"200.5,-380 200.5,-401 275.5,-401 275.5,-380 200.5,-380\"/>\n",
       "<text text-anchor=\"start\" x=\"217.5\" y=\"-388\" font-family=\"Linux libertine\" font-size=\"10.00\">(2, 128) </text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>2&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M132.38,-457.63C139.43,-449.13 147.73,-439.12 155.34,-429.94\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"158.1,-432.09 161.79,-422.16 152.71,-427.62 158.1,-432.09\"/>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>6</title>\n",
       "<polygon fill=\"aliceblue\" stroke=\"transparent\" points=\"203.5,-188 15.5,-188 15.5,-146 203.5,-146 203.5,-188\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"15.5,-146 15.5,-188 62.5,-188 62.5,-146 15.5,-146\"/>\n",
       "<text text-anchor=\"start\" x=\"31\" y=\"-170\" font-family=\"Linux libertine\" font-size=\"10.00\">cat</text>\n",
       "<text text-anchor=\"start\" x=\"20.5\" y=\"-159\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:1</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"62.5,-167 62.5,-188 110.5,-188 110.5,-167 62.5,-167\"/>\n",
       "<text text-anchor=\"start\" x=\"72.5\" y=\"-175\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"110.5,-167 110.5,-188 203.5,-188 203.5,-167 110.5,-167\"/>\n",
       "<text text-anchor=\"start\" x=\"115.5\" y=\"-175\" font-family=\"Linux libertine\" font-size=\"10.00\">2 x (2, 100, 128) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"62.5,-146 62.5,-167 110.5,-167 110.5,-146 62.5,-146\"/>\n",
       "<text text-anchor=\"start\" x=\"67.5\" y=\"-154\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"110.5,-146 110.5,-167 203.5,-167 203.5,-146 110.5,-146\"/>\n",
       "<text text-anchor=\"start\" x=\"124.5\" y=\"-154\" font-family=\"Linux libertine\" font-size=\"10.00\">(2, 100, 256) </text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;6 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>2&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M94.86,-457.9C86.17,-448 77.03,-435.35 72.5,-422 62.1,-391.37 36.32,-349.12 73.5,-224 76.36,-214.38 81.42,-204.84 86.86,-196.44\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"89.79,-198.35 92.59,-188.14 84.03,-194.38 89.79,-198.35\"/>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>4</title>\n",
       "<polygon fill=\"aliceblue\" stroke=\"transparent\" points=\"260.5,-344 86.5,-344 86.5,-302 260.5,-302 260.5,-344\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"86.5,-302 86.5,-344 149.5,-344 149.5,-302 86.5,-302\"/>\n",
       "<text text-anchor=\"start\" x=\"91.5\" y=\"-326\" font-family=\"Linux libertine\" font-size=\"10.00\">unsqueeze</text>\n",
       "<text text-anchor=\"start\" x=\"99.5\" y=\"-315\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:1</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"149.5,-323 149.5,-344 197.5,-344 197.5,-323 149.5,-323\"/>\n",
       "<text text-anchor=\"start\" x=\"159.5\" y=\"-331\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"197.5,-323 197.5,-344 260.5,-344 260.5,-323 197.5,-323\"/>\n",
       "<text text-anchor=\"start\" x=\"208.5\" y=\"-331\" font-family=\"Linux libertine\" font-size=\"10.00\">(2, 128) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"149.5,-302 149.5,-323 197.5,-323 197.5,-302 149.5,-302\"/>\n",
       "<text text-anchor=\"start\" x=\"154.5\" y=\"-310\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"197.5,-302 197.5,-323 260.5,-323 260.5,-302 197.5,-302\"/>\n",
       "<text text-anchor=\"start\" x=\"202.5\" y=\"-310\" font-family=\"Linux libertine\" font-size=\"10.00\">(2, 1, 128) </text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;4 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>3&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M177.16,-379.63C176.65,-371.82 176.05,-362.73 175.49,-354.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"178.98,-353.91 174.83,-344.16 171.99,-354.37 178.98,-353.91\"/>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>5</title>\n",
       "<polygon fill=\"aliceblue\" stroke=\"transparent\" points=\"252.5,-266 82.5,-266 82.5,-224 252.5,-224 252.5,-266\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"82.5,-224 82.5,-266 129.5,-266 129.5,-224 82.5,-224\"/>\n",
       "<text text-anchor=\"start\" x=\"87.5\" y=\"-248\" font-family=\"Linux libertine\" font-size=\"10.00\">expand</text>\n",
       "<text text-anchor=\"start\" x=\"87.5\" y=\"-237\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:1</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"129.5,-245 129.5,-266 177.5,-266 177.5,-245 129.5,-245\"/>\n",
       "<text text-anchor=\"start\" x=\"139.5\" y=\"-253\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"177.5,-245 177.5,-266 252.5,-266 252.5,-245 177.5,-245\"/>\n",
       "<text text-anchor=\"start\" x=\"188.5\" y=\"-253\" font-family=\"Linux libertine\" font-size=\"10.00\">(2, 1, 128) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"129.5,-224 129.5,-245 177.5,-245 177.5,-224 129.5,-224\"/>\n",
       "<text text-anchor=\"start\" x=\"134.5\" y=\"-232\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"177.5,-224 177.5,-245 252.5,-245 252.5,-224 177.5,-224\"/>\n",
       "<text text-anchor=\"start\" x=\"182.5\" y=\"-232\" font-family=\"Linux libertine\" font-size=\"10.00\">(2, 100, 128) </text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;5 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>4&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M171.89,-301.63C171.28,-293.82 170.56,-284.73 169.88,-276.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"173.37,-275.85 169.09,-266.16 166.39,-276.4 173.37,-275.85\"/>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;6 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>5&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M151.96,-223.63C145.53,-215.22 137.98,-205.32 131.04,-196.22\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"133.73,-193.99 124.88,-188.16 128.17,-198.23 133.73,-193.99\"/>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>7</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"transparent\" points=\"194.5,-110 24.5,-110 24.5,-68 194.5,-68 194.5,-110\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"24.5,-68 24.5,-110 71.5,-110 71.5,-68 24.5,-68\"/>\n",
       "<text text-anchor=\"start\" x=\"32.5\" y=\"-92\" font-family=\"Linux libertine\" font-size=\"10.00\">Linear</text>\n",
       "<text text-anchor=\"start\" x=\"29.5\" y=\"-81\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:1</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"71.5,-89 71.5,-110 119.5,-110 119.5,-89 71.5,-89\"/>\n",
       "<text text-anchor=\"start\" x=\"81.5\" y=\"-97\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"119.5,-89 119.5,-110 194.5,-110 194.5,-89 119.5,-89\"/>\n",
       "<text text-anchor=\"start\" x=\"124.5\" y=\"-97\" font-family=\"Linux libertine\" font-size=\"10.00\">(2, 100, 256) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"71.5,-68 71.5,-89 119.5,-89 119.5,-68 71.5,-68\"/>\n",
       "<text text-anchor=\"start\" x=\"76.5\" y=\"-76\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"119.5,-68 119.5,-89 194.5,-89 194.5,-68 119.5,-68\"/>\n",
       "<text text-anchor=\"start\" x=\"127.5\" y=\"-76\" font-family=\"Linux libertine\" font-size=\"10.00\">(2, 100, 10) </text>\n",
       "</g>\n",
       "<!-- 6&#45;&gt;7 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>6&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M109.5,-145.63C109.5,-137.82 109.5,-128.73 109.5,-120.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"113,-120.16 109.5,-110.16 106,-120.16 113,-120.16\"/>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>8</title>\n",
       "<polygon fill=\"lightyellow\" stroke=\"transparent\" points=\"181,-32 38,-32 38,0 181,0 181,-32\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"38.5,0 38.5,-32 115.5,-32 115.5,0 38.5,0\"/>\n",
       "<text text-anchor=\"start\" x=\"43.5\" y=\"-19\" font-family=\"Linux libertine\" font-size=\"10.00\">output&#45;tensor</text>\n",
       "<text text-anchor=\"start\" x=\"58.5\" y=\"-8\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:0</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"115.5,0 115.5,-32 181.5,-32 181.5,0 115.5,0\"/>\n",
       "<text text-anchor=\"start\" x=\"120.5\" y=\"-13.5\" font-family=\"Linux libertine\" font-size=\"10.00\">(2, 100, 10)</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;8 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>7&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M109.5,-67.84C109.5,-59.89 109.5,-50.66 109.5,-42.26\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"113,-42.24 109.5,-32.24 106,-42.24 113,-42.24\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7fe04036a870>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_graph = draw_graph(model, input_data=dummy_input, depth=1)\n",
    "\n",
    "model_graph.visual_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"o200k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200019"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.n_vocab"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
