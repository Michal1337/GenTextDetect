{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import spacy\n",
    "import textstat\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Download necessary NLTK data\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load SpaCy model for syntactic features\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lexical Features\n",
    "def lexical_features(text):\n",
    "    words = word_tokenize(text)\n",
    "    sentences = sent_tokenize(text)\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    unique_words = set(words)\n",
    "    \n",
    "    features = {\n",
    "        \"word_count\": len(words),\n",
    "        \"character_count\": sum(len(word) for word in words),\n",
    "        \"average_word_length\": sum(len(word) for word in words) / len(words),\n",
    "        \"sentence_count\": len(sentences),\n",
    "        \"unique_words_ratio\": len(unique_words) / len(words),\n",
    "        \"stopword_ratio\": len([word for word in words if word.lower() in stop_words]) / len(words)\n",
    "    }\n",
    "    return features\n",
    "\n",
    "# Syntactic Features\n",
    "def syntactic_features(text):\n",
    "    doc = nlp(text)\n",
    "    pos_counts = Counter([token.pos_ for token in doc])\n",
    "    \n",
    "    features = {\n",
    "        \"noun_ratio\": pos_counts.get(\"NOUN\", 0) / len(doc),\n",
    "        \"verb_ratio\": pos_counts.get(\"VERB\", 0) / len(doc),\n",
    "        \"adjective_ratio\": pos_counts.get(\"ADJ\", 0) / len(doc),\n",
    "        \"average_sentence_length\": sum(len(sent.text.split()) for sent in doc.sents) / len(list(doc.sents)),\n",
    "        \"entity_count\": len(doc.ents)\n",
    "    }\n",
    "    return features\n",
    "\n",
    "# Semantic Features\n",
    "def semantic_features(text):\n",
    "    doc = nlp(text)\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    \n",
    "    features = {\n",
    "        \"named_entities\": entities,\n",
    "        \"entity_count\": len(entities)\n",
    "    }\n",
    "    return features\n",
    "\n",
    "# Readability Scores\n",
    "def readability_features(text):\n",
    "    features = {\n",
    "        \"flesch_reading_ease\": textstat.flesch_reading_ease(text),\n",
    "        \"gunning_fog_index\": textstat.gunning_fog(text),\n",
    "        \"smog_index\": textstat.smog_index(text),\n",
    "        \"automated_readability_index\": textstat.automated_readability_index(text),\n",
    "        \"dale_chall_readbility\": textstat.dale_chall_readability_score(text),\n",
    "\n",
    "    }\n",
    "    return features\n",
    "\n",
    "# Stylometric Features\n",
    "def stylometric_features(text):\n",
    "    words = word_tokenize(text)\n",
    "    bigrams = list(nltk.bigrams(words))\n",
    "    trigrams = list(nltk.trigrams(words))\n",
    "    \n",
    "    features = {\n",
    "        \"bigram_count\": len(bigrams),\n",
    "        \"trigrams_count\": len(trigrams),\n",
    "        \"punctuation_count\": sum(1 for char in text if char in \".,;!?\")\n",
    "    }\n",
    "    return features\n",
    "\n",
    "# Combine all features for one text\n",
    "def extract_features_single_text(text):\n",
    "    features = {}\n",
    "    features.update(lexical_features(text))\n",
    "    features.update(syntactic_features(text)) # takes majority of compute time\n",
    "    #features.update(semantic_features(text))\n",
    "    features.update(readability_features(text))\n",
    "    features.update(stylometric_features(text))\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_csv_path(folder_path, recursive=False):\n",
    "    if recursive:\n",
    "        # Walk through all subdirectories\n",
    "        file_paths = [os.path.join(root, file) \n",
    "                      for root, _, files in os.walk(folder_path) \n",
    "                      for file in files if file.endswith('.csv')]\n",
    "    else:\n",
    "        # Get files in the root folder only\n",
    "        file_paths = [os.path.join(folder_path, file) \n",
    "                      for file in os.listdir(folder_path) \n",
    "                      if file.endswith('.csv')]\n",
    "    \n",
    "\n",
    "    return file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_features(texts):\n",
    "    results = []\n",
    "    for text in tqdm(texts):\n",
    "        features = extract_features_single_text(text)\n",
    "        results.append(features)\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/data_human/xsum_human.csv',\n",
       " '../data/data_human/writingprompts_human.csv',\n",
       " '../data/data_human/raid_human.csv',\n",
       " '../data/data_human/nyt_articles_human.csv',\n",
       " '../data/data_human/tweets_human.csv',\n",
       " '../data/data_human/nyt_comments_human.csv',\n",
       " '../data/data_human/reddit_human.csv',\n",
       " '../data/data_human/blogs_human.csv',\n",
       " '../data/data_human/essays_human.csv',\n",
       " '../data/data_ai/blogs/blogs_Llama-3.2-1B-Instruct.csv',\n",
       " '../data/data_ai/nyt_articles/nyt_articles_Llama-3.2-1B-Instruct.csv',\n",
       " '../data/data_ai/writingprompts/writingprompts_Llama-3.2-1B-Instruct.csv',\n",
       " '../data/data_ai/tweets/tweets_Llama-3.2-1B-Instruct.csv',\n",
       " '../data/data_ai/nyt_comments/nyt_comments_Llama-3.2-1B-Instruct.csv',\n",
       " '../data/data_ai/reddit/reddit_Llama-3.2-1B-Instruct.csv',\n",
       " '../data/data_ai/xsum/xsum_Llama-3.2-1B-Instruct.csv',\n",
       " '../data/data_ai/raid/raid_Llama-3.2-1B-Instruct.csv',\n",
       " '../data/data_ai/essays/essays_Llama-3.2-1B-Instruct.csv']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_human_path = \"../data/data_human\"\n",
    "data_ai_path = \"../data/data_ai\"\n",
    "paths = get_csv_path(data_human_path) + get_csv_path(data_ai_path, recursive=True)\n",
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in paths:\n",
    "    df = pd.read_csv(path)\n",
    "    texts = df[\"text\"]\n",
    "    df_features = calc_features(texts)\n",
    "    df_features.to_csv(path.replace(\".csv\", \"_features.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
